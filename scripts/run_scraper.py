#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
kintoneæŠ“å–å™¨è¿è¡Œè„šæœ¬
æ”¯æŒä¸åŒçš„è¿è¡Œæ¨¡å¼ï¼šæµ‹è¯•ã€å°æ‰¹é‡ã€å…¨é‡
"""

import sys
import argparse
import subprocess
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from kintone_scraper.scraper import KintoneScraper


def _inject_copy_buttons(base_output: Path) -> None:
    """åœ¨ç”Ÿæˆè¾“å‡ºåï¼Œä¸ºè¯¥è¾“å‡ºç›®å½•çš„ HTML æ³¨å…¥å¤åˆ¶æŒ‰é’®ã€‚"""
    html_dir = base_output / "html"
    if not html_dir.exists():
        print(f"âš  æœªæ‰¾åˆ° HTML ç›®å½•ï¼Œè·³è¿‡æ³¨å…¥: {html_dir}")
        return
    script = Path(__file__).parent / "inject_copy_buttons.py"
    if not script.exists():
        print(f"âš  æœªæ‰¾åˆ°æ³¨å…¥è„šæœ¬ï¼Œè·³è¿‡: {script}")
        return
    try:
        print(f"ğŸ”§ æ­£åœ¨ä¸º {html_dir} æ³¨å…¥å¤åˆ¶æŒ‰é’®...")
        proc = subprocess.run([sys.executable, str(script), str(html_dir)], capture_output=True, text=True)
        out = (proc.stdout or '').strip()
        err = (proc.stderr or '').strip()
        if out:
            print(out)
        if err:
            print(f"æ³¨å…¥æç¤º: {err}")
    except Exception as e:
        print(f"âš  æ³¨å…¥è¿‡ç¨‹å‡ºé”™: {e}")

def run_test_mode(output_dir: Path, try_external_images: bool = True):
    """æµ‹è¯•æ¨¡å¼ï¼šåªæŠ“å–1ä¸ªsectionçš„å‰3ç¯‡æ–‡ç« """
    print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šæŠ“å–å°‘é‡æ–‡ç« éªŒè¯åŠŸèƒ½")
    print("="*60)
    
    scraper = KintoneScraper(
        output_dir=output_dir,
        enable_images=True,
        try_external_images=try_external_images
    )
    
    # æµ‹è¯•ä¸¤ä¸ªsectionï¼šä¸€ä¸ªæ™®é€šçš„APIæ–‡æ¡£ï¼Œä¸€ä¸ªåŒ…å«å¤§é‡å›¾ç‰‡çš„æ’ä»¶æ–‡æ¡£
    test_sections = [
        "https://cybozudev.kf5.com/hc/kb/section/106250/",  # kintone REST API (æ™®é€šæ–‡æ¡£)
        "https://cybozudev.kf5.com/hc/kb/section/1180832/"   # æ’ä»¶å¼€å‘ (åŒ…å«101å¼ å›¾ç‰‡çš„æ–‡ç« )
    ]

    total_success = 0
    total_articles = 0

    for section_idx, section_url in enumerate(test_sections, 1):
        print(f"\nğŸ¯ Section {section_idx}/2: {section_url}")

        section = scraper._extract_section_info(section_url)
        if not section:
            print("âŒ Sectionæå–å¤±è´¥")
            continue

        print(f"âœ… Section: {section.title} ({section.article_count}ç¯‡æ–‡ç« )")
        print(f"ğŸ“‚ åˆ†ç±»: {section.category_path}")

        # æ ¹æ®sectionç±»å‹å†³å®šæŠ“å–æ•°é‡
        if "æ’ä»¶å¼€å‘" in section.title:
            # æ’ä»¶å¼€å‘sectionï¼Œä¼˜å…ˆæŠ“å–åŒ…å«å›¾ç‰‡çš„æ–‡ç« 
            max_articles = min(5, len(section.articles))  # å¤šæŠ“å‡ ç¯‡æ‰¾åˆ°å›¾ç‰‡æ–‡ç« 
            print(f"ğŸ”„ æŠ“å–å‰{max_articles}ç¯‡æ–‡ç« ï¼ˆæŸ¥æ‰¾å›¾ç‰‡æ–‡ç« ï¼‰...")
        else:
            # æ™®é€šsectionï¼ŒåªæŠ“å–å‰2ç¯‡
            max_articles = min(2, len(section.articles))
            print(f"ğŸ”„ æŠ“å–å‰{max_articles}ç¯‡æ–‡ç« ...")

        success_count = 0
        for i, article_url in enumerate(section.articles[:max_articles], 1):
            print(f"  [{i}/{max_articles}] {article_url}")

            article = scraper._extract_article_content(article_url, section)
            if article:
                print(f"    âœ… {article.title}")
                print(f"    ğŸ“Š {article.content_length}å­—ç¬¦")

                scraper._save_article_files(article, section)
                scraper.result.add_article(article, success=True)
                success_count += 1
                total_success += 1
            else:
                print(f"    âŒ æŠ“å–å¤±è´¥")
                scraper.result.failed_articles += 1

        total_articles += max_articles
        print(f"  ğŸ“Š Sectionç»“æœ: {success_count}/{max_articles} æˆåŠŸ")

    # ä¿å­˜ç»“æœ
    scraper._save_results()

    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  æˆåŠŸ: {total_success}/{total_articles}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir.absolute()}")

    # è·å–å›¾ç‰‡ä¸‹è½½ç»Ÿè®¡
    if scraper.image_downloader:
        stats = scraper.image_downloader.get_download_stats()
        print(f"  å›¾ç‰‡ä¸‹è½½: æˆåŠŸ{stats['images_downloaded']}, å¤±è´¥{stats['failed']}")
        if stats.get('attachments_downloaded', 0) > 0:
            print(f"  é™„ä»¶ä¸‹è½½: æˆåŠŸ{stats['attachments_downloaded']}")

    return total_success == total_articles

def run_small_batch(output_dir: Path, try_external_images: bool = True):
    """å°æ‰¹é‡æ¨¡å¼ï¼šæŠ“å–æ‰€æœ‰sectionï¼Œæ¯ä¸ªæœ€å¤š2ç¯‡æ–‡ç« """
    print("ğŸ“¦ å°æ‰¹é‡æ¨¡å¼ï¼šæŠ“å–æ‰€æœ‰sectionçš„éƒ¨åˆ†æ–‡ç« ")
    print("="*60)
    
    scraper = KintoneScraper(
        output_dir=output_dir,
        enable_images=True,
        try_external_images=try_external_images
    )
    
    # è·å–æ‰€æœ‰sections
    print("ğŸ” æ­£åœ¨è·å–æ‰€æœ‰sectioné“¾æ¥...")
    test_sections = scraper._extract_section_links()
    
    if not test_sections:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•section")
        return False
    
    print(f"ğŸ“‹ å‘ç° {len(test_sections)} ä¸ªsection")
    
    total_articles = 0
    max_per_section = 2
    
    for i, section_url in enumerate(test_sections, 1):
        print(f"\nğŸ¯ Section {i}/{len(test_sections)}: {section_url}")
        
        section = scraper._extract_section_info(section_url)
        if not section:
            print("âŒ Sectionæå–å¤±è´¥")
            continue
        
        print(f"âœ… Section: {section.title} ({section.article_count}ç¯‡æ–‡ç« )")
        print(f"ğŸ“‚ åˆ†ç±»: {section.category_path}")
        
        # æŠ“å–æŒ‡å®šæ•°é‡çš„æ–‡ç« 
        max_articles = min(max_per_section, len(section.articles))
        print(f"ğŸ”„ æŠ“å–å‰{max_articles}ç¯‡æ–‡ç« ...")
        
        for j, article_url in enumerate(section.articles[:max_articles], 1):
            print(f"  [{j}/{max_articles}] {article_url}")
            
            article = scraper._extract_article_content(article_url, section)
            if article:
                print(f"    âœ… {article.title} ({article.content_length}å­—ç¬¦)")
                scraper._save_article_files(article, section)
                scraper.result.add_article(article, success=True)
                total_articles += 1
            else:
                print(f"    âŒ æå–å¤±è´¥")
                scraper.result.failed_articles += 1
    
    # ä¿å­˜ç»“æœ
    scraper._save_results()
    
    print(f"\nğŸ“Š å°æ‰¹é‡ç»“æœ:")
    print(f"  æˆåŠŸæ–‡ç« : {scraper.result.successful_articles}")
    print(f"  å¤±è´¥æ–‡ç« : {scraper.result.failed_articles}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    
    if scraper.image_downloader:
        stats = scraper.image_downloader.get_download_stats()
        print(f"  å›¾ç‰‡ä¸‹è½½: æˆåŠŸ{stats['images_downloaded']}, å¤±è´¥{stats['failed']}")
        if stats.get('attachments_downloaded', 0) > 0:
            print(f"  é™„ä»¶ä¸‹è½½: æˆåŠŸ{stats['attachments_downloaded']}")

def run_tiny_batch(output_dir: Path, try_external_images: bool = True, use_api: bool = False):
    """å¾®å‹æ¨¡å¼ï¼šæŠ“å–æ‰€æœ‰sectionï¼Œæ¯ä¸ªæœ€å¤š1ç¯‡æ–‡ç« ï¼Œæ”¯æŒAPIå’Œç½‘é¡µä¸¤ç§æŠ“å–æ–¹å¼"""
    if use_api:
        print("ğŸ”¬ å¾®å‹æ¨¡å¼ï¼ˆAPIï¼‰ï¼šé€šè¿‡ API åˆ—è¡¨é©±åŠ¨æŠ“å–å°‘é‡æ–‡ç« ")
    else:
        print("ğŸ”¬ å¾®å‹æ¨¡å¼ï¼ˆç½‘é¡µï¼‰ï¼šæŠ“å–æ‰€æœ‰sectionçš„å•ç¯‡æ–‡ç« ")
    print("="*60)
    
    scraper = KintoneScraper(
        output_dir=output_dir,
        enable_images=True,
        try_external_images=try_external_images
    )
    
    if use_api:
        # APIæ¨¡å¼ï¼šä½¿ç”¨ä¿®å¤åçš„APIæŠ“å–é€»è¾‘ï¼Œä½†é™åˆ¶æ•°é‡
        if not scraper.kf5:
            print("âŒ KF5 API æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨APIæ¨¡å¼")
            return False
        
        print("ğŸ—‚ï¸  æ„å»ºåˆ†ç±»æ˜ å°„...")
        forum_mapping = scraper.kf5.build_category_mapping()
        print(f"ğŸ“‹ è·å–åˆ° {len(forum_mapping)} ä¸ªåˆ†ç±»æ˜ å°„")
        
        # æ”¹è¿›çš„APIæ¨¡å¼ï¼šä»æ›´å¤šæ–‡ç« ä¸­æŒ‰åˆ†ç±»å»é‡ï¼Œç¡®ä¿æ¯ä¸ªåˆ†ç±»éƒ½æœ‰ä»£è¡¨
        from kintone_scraper.models import Section
        from kintone_scraper.utils import make_progress, rate_limit
        from kintone_scraper.config import REQUEST_DELAY
        from urllib.parse import urljoin
        
        # æŒ‰åˆ†ç±»æ”¶é›†æ–‡ç« ï¼Œæ¯ä¸ªåˆ†ç±»æœ€å¤š1ç¯‡
        category_articles = {}
        seen_categories = set()
        
        # è·å–æ›´å¤šé¡µé¢çš„æ–‡ç« æ¥ç¡®ä¿è¦†ç›–æ‰€æœ‰åˆ†ç±»
        all_posts = []
        for page in range(1, 6):  # è·å–å‰5é¡µæ–‡ç« ï¼Œæ¯é¡µ100ç¯‡ï¼Œæ€»å…±500ç¯‡
            try:
                data = scraper.kf5.list_all_posts(page=page, per_page=100)
                posts = data.get('posts') or data.get('data') or data.get('items') or []
                if not posts:
                    break  # æ²¡æœ‰æ›´å¤šæ–‡ç« äº†
                all_posts.extend(posts)
                print(f"ğŸ“„ ç¬¬{page}é¡µ: è·å– {len(posts)} ç¯‡æ–‡ç« ")
            except Exception as e:
                print(f"âš ï¸  è·å–ç¬¬{page}é¡µæ–‡ç« å¤±è´¥: {e}")
                break
        
        print(f"API è¿”å›åŸå§‹æ–‡ç« : {len(all_posts)} ç¯‡")
        
        # æŒ‰åˆ†ç±»å»é‡ï¼Œæ¯ä¸ªåˆ†ç±»åªä¿ç•™ç¬¬ä¸€ç¯‡æ–‡ç« 
        for post in all_posts:
            forum_id = post.get('forum_id')
            forum_name = post.get('forum_name', '')
            
            # æ„å»ºåˆ†ç±»æ ‡è¯†
            if forum_id and forum_id in forum_mapping:
                category_key = forum_mapping[forum_id]['full_path']
                post['forum_name'] = forum_mapping[forum_id]['forum_name']  # ä½¿ç”¨æ˜ å°„ä¸­çš„åç§°
            elif forum_name:
                category_key = f"å…¶ä»–/{forum_name}"
            else:
                category_key = "å…¶ä»–/æœªçŸ¥"
            
            # æ¯ä¸ªåˆ†ç±»åªä¿ç•™ç¬¬ä¸€ç¯‡æ–‡ç« 
            if category_key not in seen_categories:
                seen_categories.add(category_key)
                category_articles[category_key] = post
                print(f"ğŸ“‚ {category_key}: é€‰æ‹©æ–‡ç«  {post.get('id')} - {post.get('title', '')[:50]}...")
        
        filtered_items = list(category_articles.values())
        total_articles = len(filtered_items)
        scraper.result.total_articles = total_articles
        print(f"æœ€ç»ˆæ”¶é›†æ–‡ç« : {total_articles} ç¯‡ï¼ˆè¦†ç›– {len(seen_categories)} ä¸ªåˆ†ç±»ï¼‰")

        article_progress = make_progress(total_articles, "æŠ“å–æ–‡ç« :")
        for i, it in enumerate(filtered_items, 1):
            aid = str(it.get('id') or '').strip()
            url = (it.get('url') or '').strip()
            title = (it.get('title') or '').strip()
            forum_id = it.get('forum_id')
            forum_name = it.get('forum_name', '')
            
            if not url and aid:
                url = f"/hc/kb/article/{aid}/"
            elif url and '/hc/kb/article/' not in url and aid:
                url = f"/hc/kb/article/{aid}/"
                
            if not (aid or url):
                continue
                
            article_url = urljoin(scraper.base_url, url)
            
            # æ„å»ºæ­£ç¡®çš„åˆ†ç±»ä¿¡æ¯
            category_path = "å…¶ä»–/æœªçŸ¥"
            if forum_id and forum_id in forum_mapping:
                category_path = forum_mapping[forum_id]['full_path']
            elif forum_name:
                category_path = f"å…¶ä»–/{forum_name}"
            
            article_section = Section(
                url="", 
                title=forum_name or 'æœªçŸ¥åˆ†ç±»', 
                description="", 
                articles=[], 
                category_path=category_path
            )
            
            print(f"  [{i}/{total_articles}] {article_url}")
            print(f"    ğŸ“‚ åˆ†ç±»: {category_path}")
            
            article = scraper._extract_article_content(article_url, article_section)
            if article:
                print(f"    âœ… {article.title} ({article.content_length}å­—ç¬¦)")
                scraper._save_article_files(article, article_section)
                scraper.result.add_article(article, success=True)
            else:
                print(f"    âŒ æå–å¤±è´¥")
                scraper.result.failed_articles += 1
                
            article_progress.update()
            rate_limit(REQUEST_DELAY)
            
        article_progress.finish()
        
    else:
        # ç½‘é¡µæ¨¡å¼ï¼šç±»ä¼¼smallæ¨¡å¼ä½†æ¯ä¸ªsectionåªæŠ“1ç¯‡
        print("ğŸ” æ­£åœ¨è·å–æ‰€æœ‰sectioné“¾æ¥...")
        test_sections = scraper._extract_section_links()
        
        if not test_sections:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•section")
            return False
        
        print(f"ğŸ“‹ å‘ç° {len(test_sections)} ä¸ªsection")
        
        total_articles = 0
        max_per_section = 1  # tinyæ¨¡å¼æ¯ä¸ªsectionåªæŠ“1ç¯‡
        
        for i, section_url in enumerate(test_sections, 1):
            print(f"\nğŸ¯ Section {i}/{len(test_sections)}: {section_url}")
            
            section = scraper._extract_section_info(section_url)
            if not section:
                print("âŒ Sectionæå–å¤±è´¥")
                continue
            
            print(f"âœ… Section: {section.title} ({section.article_count}ç¯‡æ–‡ç« )")
            print(f"ğŸ“‚ åˆ†ç±»: {section.category_path}")
            
            # æŠ“å–æŒ‡å®šæ•°é‡çš„æ–‡ç« 
            max_articles = min(max_per_section, len(section.articles))
            print(f"ğŸ”„ æŠ“å–å‰{max_articles}ç¯‡æ–‡ç« ...")
            
            for j, article_url in enumerate(section.articles[:max_articles], 1):
                print(f"  [{j}/{max_articles}] {article_url}")
                
                article = scraper._extract_article_content(article_url, section)
                if article:
                    print(f"    âœ… {article.title} ({article.content_length}å­—ç¬¦)")
                    scraper._save_article_files(article, section)
                    scraper.result.add_article(article, success=True)
                    total_articles += 1
                else:
                    print(f"    âŒ æå–å¤±è´¥")
                    scraper.result.failed_articles += 1
    
    # ä¿å­˜ç»“æœ
    scraper._save_results()
    
    print(f"\nğŸ“Š å¾®å‹æ¨¡å¼ç»“æœ:")
    print(f"  æˆåŠŸæ–‡ç« : {scraper.result.successful_articles}")
    print(f"  å¤±è´¥æ–‡ç« : {scraper.result.failed_articles}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    
    if scraper.image_downloader:
        stats = scraper.image_downloader.get_download_stats()
        print(f"  å›¾ç‰‡ä¸‹è½½: æˆåŠŸ{stats['images_downloaded']}, å¤±è´¥{stats['failed']}")
        if stats.get('attachments_downloaded', 0) > 0:
            print(f"  é™„ä»¶ä¸‹è½½: æˆåŠŸ{stats['attachments_downloaded']}")
    
    return True

def run_full_scrape(output_dir: Path, try_external_images: bool = True):
    """å…¨é‡æ¨¡å¼ï¼šæŠ“å–æ‰€æœ‰æ–‡æ¡£"""
    print("ğŸŒ å…¨é‡æ¨¡å¼ï¼šæŠ“å–æ‰€æœ‰kintoneæ–‡æ¡£")
    print("="*60)
    print("âš ï¸  è¿™å°†éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š")
    
    confirm = input("ç¡®è®¤å¼€å§‹å…¨é‡æŠ“å–ï¼Ÿ(y/N): ")
    if confirm.lower() != 'y':
        print("âŒ ç”¨æˆ·å–æ¶ˆ")
        return
    
    scraper = KintoneScraper(
        output_dir=output_dir,
        enable_images=True,
        try_external_images=try_external_images
    )
    
    # è¿è¡Œå®Œæ•´æŠ“å–
    result = scraper.scrape_all()
    
    print(f"\nğŸ“Š å…¨é‡æŠ“å–å®Œæˆ:")
    print(f"  æˆåŠŸæ–‡ç« : {result.successful_articles}")
    print(f"  å¤±è´¥æ–‡ç« : {result.failed_articles}")
    print(f"  æˆåŠŸç‡: {result.get_success_rate():.1%}")
    print(f"  è€—æ—¶: {result.duration}")
    
    if scraper.image_downloader:
        stats = scraper.image_downloader.get_download_stats()
        print(f"  å›¾ç‰‡ä¸‹è½½: æˆåŠŸ{stats['images_downloaded']}, å¤±è´¥{stats['failed']}")
        if stats.get('attachments_downloaded', 0) > 0:
            print(f"  é™„ä»¶ä¸‹è½½: æˆåŠŸ{stats['attachments_downloaded']}")

def main():
    parser = argparse.ArgumentParser(description="kintoneæ–‡æ¡£æŠ“å–å™¨")
    parser.add_argument(
        "mode",
        choices=["test", "small", "tiny", "full"],
        help="è¿è¡Œæ¨¡å¼: test(æµ‹è¯•3ç¯‡), small(å°æ‰¹é‡), tiny(æ¯section1ç¯‡), full(å…¨é‡)"
    )
    parser.add_argument(
        "-o", "--output",
        type=Path,
        default=Path("output"),
        help="è¾“å‡ºç›®å½• (é»˜è®¤: output)"
    )
    parser.add_argument(
        "--skip-external-images",
        action="store_true",
        help="è·³è¿‡å¤–éƒ¨å›¾åºŠçš„å›¾ç‰‡ä¸‹è½½ (é»˜è®¤ä¼šå°è¯•ä¸‹è½½)"
    )
    parser.add_argument(
        "--use-api",
        action="store_true",
        help="ä½¿ç”¨ KF5 API åˆ—è¡¨é©±åŠ¨æŠ“å–ï¼ˆæ›´å®Œæ•´ï¼Œä¸æ˜“æ¼æ–‡ï¼‰"
    )
    parser.add_argument(
        "--no-skip-existing",
        action="store_true",
        help="ä¸è·³è¿‡å·²å­˜åœ¨çš„æ–‡ç« ï¼ˆé»˜è®¤ä¼šè·³è¿‡ä»¥èŠ‚çœæ—¶é—´ï¼‰"
    )
    
    args = parser.parse_args()
    
    # æ ¹æ®æ¨¡å¼åˆ›å»ºä¸åŒçš„è¾“å‡ºç›®å½•
    if args.output == Path("output"):  # ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•
        output_dir = Path(f"output_{args.mode}")
    else:  # ç”¨æˆ·æŒ‡å®šäº†è¾“å‡ºç›®å½•
        output_dir = args.output / args.mode
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print(f"ğŸ¯ è¿è¡Œæ¨¡å¼: {args.mode}")
    print()
    
    try:
        if args.mode == "test":
            success = run_test_mode(output_dir, not args.skip_external_images)
            if success:
                print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
                print("å¯ä»¥å°è¯• small æ¨¡å¼")
            else:
                print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é—®é¢˜")
        
        elif args.mode == "small":
            run_small_batch(output_dir, not args.skip_external_images)
            print(f"\nğŸ‰ å°æ‰¹é‡å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
            print("å¦‚æœæ•ˆæœæ»¡æ„ï¼Œå¯ä»¥è¿è¡Œ full æ¨¡å¼")

        elif args.mode == "tiny":
            success = run_tiny_batch(
                output_dir, 
                not args.skip_external_images, 
                use_api=args.use_api
            )
            if success:
                print(f"\nğŸ‰ å¾®å‹æ¨¡å¼å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
                if args.use_api:
                    print("ğŸ’¡ APIæ¨¡å¼æµ‹è¯•å®Œæˆï¼Œå¯ä»¥éªŒè¯åˆ†ç±»æ˜¯å¦æ­£ç¡®")
                else:
                    print("ğŸ’¡ ç½‘é¡µæ¨¡å¼æµ‹è¯•å®Œæˆï¼Œå¯ä»¥å°è¯• --use-api æµ‹è¯•APIæ¨¡å¼")
            else:
                print("\nâŒ å¾®å‹æ¨¡å¼å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")

        elif args.mode == "full":
            if args.use_api:
                print("ğŸŒ å…¨é‡æ¨¡å¼ï¼ˆAPIï¼‰ï¼šé€šè¿‡ API åˆ—è¡¨é©±åŠ¨æŠ“å–")
                scraper = KintoneScraper(
                    output_dir=output_dir,
                    enable_images=True,
                    try_external_images=not args.skip_external_images,
                    skip_existing=(not args.no_skip_existing)
                )
                res = scraper.scrape_all_via_api()
                print(f"\nğŸ“Š å…¨é‡æŠ“å–å®Œæˆ(åŸºäºAPI): æˆåŠŸ{res.successful_articles}/{res.total_articles}")
            else:
                # é API è·¯å¾„ä¸‹ä¹Ÿåº”ç”¨è·³è¿‡é€»è¾‘
                scraper = KintoneScraper(
                    output_dir=output_dir,
                    enable_images=True,
                    try_external_images=not args.skip_external_images,
                    skip_existing=(not args.no_skip_existing)
                )
                # å¤ç”¨ scrape_all çš„å®ç°
                res = scraper.scrape_all()
                print(f"\nğŸ“Š å…¨é‡æŠ“å–å®Œæˆ: æˆåŠŸ{res.successful_articles}/{res.total_articles}")
            print(f"\nğŸ‰ å…¨é‡æŠ“å–å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    # ç»Ÿä¸€åœ¨ç»“æŸåæ³¨å…¥å¤åˆ¶æŒ‰é’®
    _inject_copy_buttons(output_dir)

if __name__ == "__main__":
    main()
