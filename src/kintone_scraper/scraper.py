"""æ ¸å¿ƒæŠ“å–å™¨"""

import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup, Tag

from .config import (
    BASE_URL, DEFAULT_HEADERS, DEFAULT_OUTPUT_DIR,
    REQUEST_DELAY, REQUEST_TIMEOUT, SELECTORS, get_category_path, BILIBILI_VIDEO_MODE, ARTICLE_WORKERS
)
from .models import Article, Category, ScrapingResult, Section
from .utils import rate_limit, make_progress
from .image_downloader import ImageDownloader, HTMLGenerator
try:
    from .kf5_api import KF5HelpCenterClient  # optional API client
except Exception:
    KF5HelpCenterClient = None  # type: ignore


logger = logging.getLogger(__name__)


class KintoneScraper:
    """kintoneæ–‡æ¡£æŠ“å–å™¨"""
    
    def __init__(self, output_dir: Path = DEFAULT_OUTPUT_DIR, base_url: str = BASE_URL, enable_images: bool = True, try_external_images: bool = False, bilibili_mode: Optional[str] = None, skip_existing: bool = True, article_workers: Optional[int] = None):
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.enable_images = enable_images
        self.try_external_images = try_external_images
        self.bilibili_mode = bilibili_mode or BILIBILI_VIDEO_MODE
        self.skip_existing = skip_existing  # æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡ç« HTML
        self.article_workers = max(1, article_workers or ARTICLE_WORKERS)
        
        # åˆ›å»ºsession
        self.session = requests.Session()
        self.session.headers.update(DEFAULT_HEADERS)
        self._thread_local = threading.local()
        self._thread_local.session = self.session
        
        # è·Ÿè¸ªå·²è®¿é—®çš„URL
        self.visited_urls: Set[str] = set()
        self._visited_lock = threading.Lock()
        
        # æŠ“å–ç»“æœ
        self.result = ScrapingResult()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–å›¾ç‰‡ä¸‹è½½å™¨å’ŒHTMLç”Ÿæˆå™¨
        if self.enable_images:
            self.image_downloader = ImageDownloader(self.base_url, self.output_dir, self.try_external_images, self.bilibili_mode)
            self.html_generator = HTMLGenerator(self.output_dir)
        else:
            self.image_downloader = None
            self.html_generator = None

        # åˆå§‹åŒ–å¯é€‰çš„ KF5 API å®¢æˆ·ç«¯ï¼Œç”¨äºå¯ŒåŒ–å…ƒæ•°æ®
        self.kf5 = None
        if KF5HelpCenterClient is not None:
            try:
                self.kf5 = KF5HelpCenterClient()
                logger.info("KF5 API å·²å¯ç”¨ç”¨äºå…ƒæ•°æ®å¯ŒåŒ–")
            except Exception as e:
                logger.warning(f"KF5 API åˆå§‹åŒ–å¤±è´¥ï¼Œå¿½ç•¥ API å¯ŒåŒ–: {e}")
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
    
    def _setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.output_dir / "scraper.log"
        
        # é…ç½®æ—¥å¿—æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        
        # é…ç½®logger
        logger.setLevel(logging.INFO)
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

    def _extract_article_id(self, url: str) -> Optional[str]:
        """ä»æ–‡ç« URLä¸­æå–IDï¼Œå¦‚ /hc/kb/article/211164/ -> 211164"""
        try:
            import re
            m = re.search(r"/hc/kb/article/(\d+)/", url)
            return m.group(1) if m else None
        except Exception:
            return None

    def _existing_html_for_id(self, article_id: str) -> Optional[Path]:
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥æ–‡ç« IDç”Ÿæˆçš„HTMLæ–‡ä»¶ï¼Œè¿”å›è·¯å¾„æˆ–None"""
        try:
            html_root = self.output_dir / "html"
            if not html_root.exists():
                return None
            # é€’å½’æŸ¥æ‰¾ä»»æ„åˆ†ç±»ä¸‹ä»¥ ID_ å¼€å¤´çš„æ–‡ä»¶
            for path in html_root.rglob(f"{article_id}_*.html"):
                if path.is_file():
                    return path
            return None
        except Exception:
            return None
    
    def _get_thread_session(self) -> requests.Session:
        """ä¸ºå½“å‰çº¿ç¨‹æä¾›å¸¦é»˜è®¤å¤´çš„session"""
        session = getattr(self._thread_local, 'session', None)
        if session is None:
            session = requests.Session()
            session.headers.update(DEFAULT_HEADERS)
            self._thread_local.session = session
        return session

    def _get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹"""
        with self._visited_lock:
            if url in self.visited_urls:
                logger.debug(f"è·³è¿‡å·²è®¿é—®çš„URL: {url}")
                return None
        
        try:
            logger.info(f"è®¿é—®: {url}")
            session = self._get_thread_session()
            response = session.get(url, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            with self._visited_lock:
                self.visited_urls.add(url)
            return BeautifulSoup(response.text, 'html.parser')
            
        except requests.RequestException as e:
            logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    def _extract_section_links(self) -> List[str]:
        """æå–æ‰€æœ‰sectioné“¾æ¥ï¼ˆé€šè¿‡é¦–é¡µå’Œå„åˆ†ç±»é¡µï¼‰"""
        logger.info("å¼€å§‹æå–sectioné“¾æ¥...")

        soup = self._get_page_content(self.base_url)
        if not soup:
            return []

        section_links: Set[str] = set()

        # 1) ä»é¦–é¡µå·²å±•ç¤ºçš„sectionçš„â€œæŸ¥çœ‹å…¨éƒ¨æ–‡æ¡£â€é“¾æ¥æŠ“ä¸€éï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
        try:
            more_links = soup.select(SELECTORS['section_links'])
            for link in more_links:
                href = link.get('href')
                if href and '/hc/kb/section/' in href:
                    full_url = urljoin(self.base_url, href)
                    section_links.add(full_url)
        except Exception as e:
            logger.warning(f"é¦–é¡µæå–sectioné“¾æ¥å¤±è´¥: {e}")

        # 2) æå–æ‰€æœ‰åˆ†ç±»é“¾æ¥ï¼Œå†è¿›å…¥åˆ†ç±»é¡µæå–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰section
        try:
            category_links = soup.select(SELECTORS.get('category_links', 'a[href*="/hc/kb/category/"]'))
            category_urls = []
            for a in category_links:
                href = a.get('href')
                if href and '/hc/kb/category/' in href:
                    category_urls.append(urljoin(self.base_url, href))
            category_urls = list(dict.fromkeys(category_urls))  # å»é‡å¹¶ä¿æŒé¡ºåº

            logger.info(f"å‘ç° {len(category_urls)} ä¸ªä¸»åˆ†ç±»ï¼Œé€ä¸€æå–å…¶Sections")
            for cat_url in category_urls:
                cat_soup = self._get_page_content(cat_url)
                if not cat_soup:
                    continue
                sec_as = cat_soup.select('a[href*="/hc/kb/section/"]')
                for a in sec_as:
                    href = a.get('href')
                    if href and '/hc/kb/section/' in href:
                        section_links.add(urljoin(self.base_url, href))
                rate_limit(REQUEST_DELAY)
        except Exception as e:
            logger.warning(f"åˆ†ç±»é¡µæå–sectioné“¾æ¥å¤±è´¥: {e}")

        links = list(section_links)
        logger.info(f"å…±å‘ç° {len(links)} ä¸ªsection")
        return links
    
    def _extract_section_info(self, section_url: str) -> Optional[Section]:
        """æå–sectionä¿¡æ¯å’Œæ–‡ç« åˆ—è¡¨"""
        soup = self._get_page_content(section_url)
        if not soup:
            return None
        
        # è·å–sectionæ ‡é¢˜ï¼Œä¼˜å…ˆä»é¡µé¢titleä¸­æå–
        section_title = ""
        
        # é¦–å…ˆå°è¯•ä»é¡µé¢titleä¸­æå–
        title_elem = soup.find('title')
        if title_elem:
            title_text = title_elem.get_text(strip=True)
            # æ ‡é¢˜æ ¼å¼é€šå¸¸æ˜¯ "Sectionå - cybozu - cybozuå¼€å‘è€…ç½‘ç«™"
            parts = title_text.split(' - ')
            if len(parts) > 0:
                section_title = parts[0].strip()
        
        # å¦‚æœtitleæå–å¤±è´¥ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
        if not section_title:
            title_selectors = [
                'h1.section-title',
                '.section-header h1', 
                'h1',
                '.breadcrumb li:last-child'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    section_title = title_elem.get_text(strip=True)
                    if section_title:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                        break
        
        # è·å–æè¿°
        description = ""
        desc_elem = soup.find('p', class_='section-description')
        if desc_elem:
            description = desc_elem.get_text(strip=True)
        
        # æå–æ–‡ç« é“¾æ¥
        article_links = []
        for link in soup.select(SELECTORS['article_links']):
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                article_links.append(full_url)
        
        # è·å–åˆ†ç±»è·¯å¾„ï¼šä¼˜å…ˆä½¿ç”¨é¡µé¢é¢åŒ…å±‘ä¸­çš„ä¸»åˆ†ç±»/å­åˆ†ç±»
        category_path = ""
        try:
            breadcrumb = soup.select_one(SELECTORS.get('breadcrumbs', SELECTORS.get('breadcrumb', '')))
            if breadcrumb:
                # é¢åŒ…å±‘é€šå¸¸ç±»ä¼¼ï¼š é¦–é¡µ > å¼€å‘èŒƒä¾‹ > è‡ªå®šä¹‰å¼€å‘
                items = [li.get_text(strip=True) for li in breadcrumb.find_all('li')]
                if len(items) >= 3:
                    main_cat = items[1]
                    sub_cat = items[-1]
                    if main_cat and sub_cat:
                        category_path = f"{main_cat}/{sub_cat}"
        except Exception:
            pass

        # å›é€€åˆ°é™æ€æ˜ å°„
        if not category_path:
            category_path = get_category_path(section_title)
        
        section = Section(
            url=section_url,
            title=section_title,
            description=description,
            articles=article_links,
            category_path=category_path
        )
        # article_countä¼šåœ¨__post_init__ä¸­è‡ªåŠ¨è®¡ç®—
        
        logger.info(f"Section: {section_title} - {len(article_links)} ç¯‡æ–‡ç«  - åˆ†ç±»: {category_path}")
        return section
    
    def _extract_article_content(self, article_url: str, section: Section) -> Optional[Article]:
        """æå–æ–‡ç« å†…å®¹"""
        soup = self._get_page_content(article_url)
        if not soup:
            return None
        
        article = Article(url=article_url, section_title=section.title)
        
        try:
            # é¦–å…ˆå°è¯•æ‰¾åˆ°articleæ ‡ç­¾
            article_elem = soup.select_one('article')
            if article_elem:
                # åœ¨å¤„ç†å›¾ç‰‡å‰ï¼Œä¼˜å…ˆè§£æé¢åŒ…å±‘ç”¨äºç¡®å®šåˆ†ç±»æ·±åº¦ï¼ˆå½±å“å›¾ç‰‡ç›¸å¯¹è·¯å¾„ï¼‰
                processing_category = section.category_path
                if not processing_category:
                    try:
                        breadcrumb = soup.select_one(SELECTORS.get('breadcrumbs', SELECTORS.get('breadcrumb', '')))
                        if breadcrumb:
                            links = breadcrumb.find_all('a')
                            texts = [a.get_text(strip=True) for a in links]
                            # æœŸæœ›: [é¦–é¡µ, ä¸»åˆ†ç±»]
                            if len(texts) >= 2:
                                main_cat = texts[-1] if len(texts) == 2 else texts[1]
                                # æœ€åä¸€ä¸ª li å¯èƒ½æ˜¯çº¯æ–‡æœ¬å­åˆ†ç±»
                                last_li = breadcrumb.find_all('li')[-1]
                                sub_cat = last_li.get_text(strip=True) if last_li else ''
                                if main_cat and sub_cat:
                                    processing_category = f"{main_cat}/{sub_cat}"
                    except Exception:
                        pass
                # ä»articleå†…éƒ¨æå–æ ‡é¢˜
                title_elem = article_elem.find(['h1', 'h2', 'h3'])
                if title_elem:
                    article.title = title_elem.get_text(strip=True)
                # è‹¥å·²è§£æå‡ºå®Œæ•´åˆ†ç±»è·¯å¾„ï¼Œç›´æ¥èµ‹ç»™æ–‡ç« åˆ†ç±»ï¼Œç¡®ä¿ä¿å­˜ç›®å½•æ­£ç¡®
                if processing_category:
                    article.category = processing_category
                
                # å¤„ç†å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.enable_images and self.image_downloader:
                    # å¤„ç†HTMLä¸­çš„å›¾ç‰‡ï¼Œä¸‹è½½å¹¶æ›´æ–°é“¾æ¥
                    processed_html, downloaded_images = self.image_downloader.process_html_images(
                        str(article_elem), article.title, article.url, processing_category or section.category_path, processing_category or section.category_path
                    )
                    article.html_content = processed_html
                    
                    # è®°å½•ä¸‹è½½çš„å›¾ç‰‡ä¿¡æ¯
                    if downloaded_images:
                        article.image_paths = downloaded_images  # ä¿å­˜å›¾ç‰‡è·¯å¾„ä¿¡æ¯
                        logger.info(f"æ–‡ç«  '{article.title}' ä¸‹è½½äº† {len(downloaded_images)} å¼ å›¾ç‰‡")
                else:
                    # å³ä½¿ä¸å¤„ç†å›¾ç‰‡ï¼Œä¹Ÿéœ€è¦å¤„ç†é“¾æ¥
                    if self.image_downloader:
                        processed_html, _ = self.image_downloader.process_html_images(
                            str(article_elem), article.title, article.url, section.category_path, section.category_path
                        )
                        article.html_content = processed_html
                    else:
                        article.html_content = str(article_elem)
                
                # æ¸…ç†æ–‡ç« å†…å®¹ï¼Œç§»é™¤ä¸éœ€è¦çš„å¯¼èˆªå’Œäº’åŠ¨å…ƒç´ 
                article_copy = BeautifulSoup(article.html_content, 'html.parser')
                self._clean_article_content(article_copy)
                article.html_content = str(article_copy)
                
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼ï¼Œæå–çº¯æ–‡æœ¬
                # éœ€è¦é‡æ–°è§£æåŸå§‹çš„articleå…ƒç´ æ¥è·å–çº¯æ–‡æœ¬ï¼Œå› ä¸ºprocessed_htmlå¯èƒ½åŒ…å«ä¿®æ”¹åçš„é“¾æ¥
                text_copy = BeautifulSoup(str(article_elem), 'html.parser')
                self._clean_article_content(text_copy)
                for script in text_copy.find_all(['script', 'style']):
                    script.decompose()
                
                # æå–çº¯æ–‡æœ¬
                article.content = text_copy.get_text(strip=True)
                # é‡æ–°è®¡ç®—å†…å®¹é•¿åº¦
                article.content_length = len(article.content)
            else:
                # å¦‚æœæ²¡æœ‰articleæ ‡ç­¾ï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘ä½œä¸ºå¤‡ç”¨
                # æå–æ ‡é¢˜
                for selector in SELECTORS['title']:
                    title_elem = soup.select_one(selector)
                    if title_elem:
                        article.title = title_elem.get_text(strip=True)
                        break
                
                # æå–å†…å®¹ - ä½¿ç”¨å¤‡ç”¨é€‰æ‹©å™¨
                fallback_selectors = ['.article-content', '.kb-article-content', '.content-body', '.main-content']
                for selector in fallback_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        article.html_content = str(content_elem)
                        
                        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                        for script in content_elem.find_all(['script', 'style']):
                            script.decompose()
                        
                        article.content = content_elem.get_text(strip=True)
                        # é‡æ–°è®¡ç®—å†…å®¹é•¿åº¦
                        article.content_length = len(article.content)
                        break
            
            # æå–åˆ†ç±»ä¿¡æ¯ï¼ˆè‹¥ä¸Šé¢æœªèƒ½é€šè¿‡processing_categoryè®¾ç½®ï¼‰
            if not getattr(article, 'category', None):
                breadcrumb = soup.select_one(SELECTORS.get('breadcrumbs', SELECTORS.get('breadcrumb', '')))
                if breadcrumb:
                    # æœŸæœ›ï¼š é¦–é¡µ > ä¸»åˆ†ç±» > å­åˆ†ç±»
                    links = breadcrumb.find_all('a')
                    texts = [a.get_text(strip=True) for a in links]
                    if len(texts) >= 2:
                        main_cat = texts[-1] if len(texts) == 2 else texts[1]
                        last_li = breadcrumb.find_all('li')[-1]
                        sub_cat = last_li.get_text(strip=True) if last_li else ''
                        if main_cat and sub_cat:
                            article.category = f"{main_cat}/{sub_cat}"
            
            # å¦‚æœæ²¡æœ‰ä»é¢åŒ…å±‘è·å–åˆ°åˆ†ç±»ï¼Œä½¿ç”¨sectionçš„åˆ†ç±»
            if not article.category:
                article.category = section.category_path.split('/')[-1]
            
            # æå–æ›´æ–°æ—¶é—´
            time_elem = soup.select_one(SELECTORS['last_updated'])
            if time_elem:
                datetime_attr = time_elem.get('datetime')
                if datetime_attr and isinstance(datetime_attr, str):
                    article.last_updated = datetime_attr
                else:
                    article.last_updated = time_elem.get_text(strip=True)
            
            if article.title:
                logger.debug(f"æˆåŠŸæå–æ–‡ç« : {article.title}")
                return article
            else:
                logger.warning(f"æ–‡ç« æ— æ ‡é¢˜: {article_url}")
                return None
                
        except Exception as e:
            logger.error(f"æå–æ–‡ç« å†…å®¹å¤±è´¥ {article_url}: {e}")
            return None
    
    def _clean_article_content(self, soup: BeautifulSoup) -> None:
        """æ¸…ç†æ–‡ç« å†…å®¹ï¼Œç§»é™¤å¯¼èˆªå’Œäº’åŠ¨å…ƒç´ """
        # ç§»é™¤å¸¸è§çš„å¯¼èˆªå’Œäº’åŠ¨å…ƒç´ 
        selectors_to_remove = [
            # é¡µé¢åº•éƒ¨åŒºåŸŸ
            'footer',
            '.footer',
            
            # ä¸Šä¸€ç¯‡/ä¸‹ä¸€ç¯‡å¯¼èˆª
            '.article-nav',
            '.article-navigation',
            '.prev-next',
            '.pagination',
            '[class*="prev"]',
            '[class*="next"]',
            
            # è¯„åˆ†å’Œåé¦ˆ
            '.rating',
            '.feedback',
            '.helpful',
            '.vote',
            '[class*="helpful"]',
            '[class*="vote"]',
            '[class*="rating"]',
            
            # ç¤¾äº¤åˆ†äº«
            '.share',
            '.social',
            '[class*="share"]',
            '[class*="social"]',
            
            # è¯„è®ºåŒº
            '.comments',
            '.comment',
            '[class*="comment"]',
            
            # å…¶ä»–å¸¸è§çš„å¯¼èˆªå…ƒç´ 
            '.breadcrumb',
            '.sidebar',
            '.related',
            '.tags',
            '.category-nav'
        ]
        
        # ç§»é™¤åŒ¹é…çš„å…ƒç´ 
        for selector in selectors_to_remove:
            for elem in soup.select(selector):
                elem.decompose()
        
        # ç§»é™¤åŒ…å«ç‰¹å®šæ–‡æœ¬çš„å…ƒç´ ï¼ˆæ›´ç²¾ç¡®çš„æ¸…ç†ï¼‰
        texts_to_remove = [
            'ä¸Šä¸€ç¯‡',
            'ä¸‹ä¸€ç¯‡',
            'æœ‰å¸®åŠ©',
            'äººè§‰å¾—æœ‰å¸®åŠ©',
            'è§‰å¾—æœ‰å¸®åŠ©',
            'åˆ†äº«',
            'æ”¶è—',
            'ç‚¹èµ',
            'è¯„è®º',
            'ç›¸å…³æ–‡ç« '
        ]
        
        # æŸ¥æ‰¾åŒ…å«è¿™äº›æ–‡æœ¬çš„å…ƒç´ å¹¶ç§»é™¤å…¶çˆ¶å®¹å™¨
        for text in texts_to_remove:
            # æŸ¥æ‰¾åŒ…å«ç‰¹å®šæ–‡æœ¬çš„å…ƒç´ 
            for elem in soup.find_all(text=lambda t: t is not None and isinstance(t, str) and text in t):
                parent = elem.parent
                if parent and parent.name:
                    # æ£€æŸ¥çˆ¶å…ƒç´ æ˜¯å¦åº”è¯¥è¢«ç§»é™¤
                    parent_text = parent.get_text(strip=True)
                    if len(parent_text) < 200:  # åªç§»é™¤çŸ­æ–‡æœ¬çš„å®¹å™¨ï¼Œé¿å…è¯¯åˆ æ­£æ–‡
                        logger.debug(f"ç§»é™¤å¯¼èˆªå…ƒç´ : {parent_text[:50]}...")
                        parent.decompose()
                        break
    
    def _save_article_files(self, article: Article, section: Section) -> None:
        """ä¿å­˜æ–‡ç« æ–‡ä»¶"""
        if not article.title:
            return
        
        # è·å–åˆ†ç±»è·¯å¾„ (æœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºè°ƒè¯•)
        # category_parts = section.category_path.split('/')
        
        # ç”ŸæˆHTMLæ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨å›¾ç‰‡åŠŸèƒ½ï¼‰
        if self.enable_images and self.html_generator and article.html_content:
            try:
                # è®¾ç½®æ–‡ç« çš„åˆ†ç±»è·¯å¾„ä¾›HTMLç”Ÿæˆå™¨ä½¿ç”¨
                article.category = section.category_path
                
                html_file = self.html_generator.generate_article_html(
                    article, 
                    article.html_content,
                    []  # å›¾ç‰‡ä¿¡æ¯å·²ç»åœ¨html_contentä¸­æ›´æ–°äº†é“¾æ¥
                )
                
                if html_file:
                    logger.debug(f"HTMLæ–‡ä»¶å·²ç”Ÿæˆ: {html_file}")
                    
            except Exception as e:
                logger.error(f"ç”ŸæˆHTMLæ–‡ä»¶å¤±è´¥ {article.title}: {e}")
    
    def _organize_by_categories(self, sections: List[Section]) -> List[Category]:
        """æŒ‰åˆ†ç±»ç»„ç»‡sections"""
        categories_dict: Dict[str, Category] = {}
        
        for section in sections:
            category_path = section.category_path
            main_category = category_path.split('/')[0]
            
            if main_category not in categories_dict:
                categories_dict[main_category] = Category(
                    name=main_category,
                    path=main_category
                )
            
            categories_dict[main_category].add_section(section)
        
        return list(categories_dict.values())
    
    def _scrape_single_article(self, section: Section, article_url: str) -> Optional[Article]:
        """åœ¨å·¥ä½œçº¿ç¨‹ä¸­æŠ“å–å•ç¯‡æ–‡ç« """
        try:
            return self._extract_article_content(article_url, section)
        except Exception as e:
            logger.error(f"æŠ“å–æ–‡ç« å¼‚å¸¸ {article_url}: {e}")
            return None

    def _process_article_tasks(self, tasks: List[Tuple[Section, str]], article_progress: Any) -> None:
        """ä½¿ç”¨çº¿ç¨‹æ± æŠ“å–ä»»åŠ¡åˆ—è¡¨å¹¶æ›´æ–°ç»“æœ"""
        if not tasks:
            return
        delay = REQUEST_DELAY / max(1, self.article_workers)
        logger.info(f"ä½¿ç”¨ {self.article_workers} ä¸ªçº¿ç¨‹æŠ“å– {len(tasks)} ç¯‡æ–‡ç« ")
        with ThreadPoolExecutor(max_workers=self.article_workers) as executor:
            future_to_task = {
                executor.submit(self._scrape_single_article, section, article_url): (section, article_url)
                for section, article_url in tasks
            }
            for future in as_completed(future_to_task):
                section, article_url = future_to_task[future]
                article = None
                try:
                    article = future.result()
                except Exception as exc:
                    logger.error(f"æ–‡ç« æŠ“å–å¤±è´¥ {article_url}: {exc}")
                if article:
                    self.result.add_article(article, success=True)
                    self._save_article_files(article, section)
                else:
                    self.result.failed_articles += 1
                    detail = f"{section.title or 'æœªçŸ¥åˆ†ç±»'} -> {article_url}"
                    self.result.failed_details.append(detail)
                    logger.warning(f"æ–‡ç« æŠ“å–å¤±è´¥: {detail}")
                article_progress.update()
                if delay > 0:
                    rate_limit(delay)


    def scrape_all(self, section_article_limit: Optional[int] = None) -> ScrapingResult:
        """æŠ“å–æ‰€æœ‰æ–‡æ¡£"""
        logger.info("="*60)
        logger.info("å¼€å§‹æŠ“å–kintoneå¼€å‘è€…æ–‡æ¡£")
        logger.info("="*60)

        # é‡ç½®ç»“æœä¸è®¿é—®è®°å½•ï¼Œç¡®ä¿å¤šæ¬¡è¿è¡Œä¸€è‡´
        self.result = ScrapingResult()
        self.visited_urls.clear()

        try:
            # 1. æå–æ‰€æœ‰sectioné“¾æ¥
            section_links = self._extract_section_links()
            if not section_links:
                logger.error("æœªæ‰¾åˆ°ä»»ä½•sectioné“¾æ¥")
                return self.result

            sections: List[Section] = []
            section_progress = make_progress(len(section_links), "å¤„ç†Sections:")
            total_articles = 0

            for section_url in section_links:
                section = self._extract_section_info(section_url)
                if section:
                    if section_article_limit is not None:
                        section.articles = section.articles[:section_article_limit]
                    section.article_count = len(section.articles)
                    total_articles += section.article_count
                    sections.append(section)

                section_progress.update()
                rate_limit(REQUEST_DELAY)

            section_progress.finish()

            self.result.total_sections = len(sections)
            self.result.total_articles = total_articles

            # 3. æŒ‰åˆ†ç±»ç»„ç»‡
            self.result.categories = self._organize_by_categories(sections)

            # 4. å‡†å¤‡æŠ“å–
            logger.info(f"å¼€å§‹æŠ“å– {self.result.total_articles} ç¯‡æ–‡ç« ...")
            article_progress = make_progress(self.result.total_articles or 1, "æŠ“å–æ–‡ç« :")

            tasks: List[Tuple[Section, str]] = []
            for section in sections:
                for article_url in section.articles:
                    if self.skip_existing:
                        aid = self._extract_article_id(article_url) or ""
                        if aid:
                            existed = self._existing_html_for_id(aid)
                            if existed:
                                logger.info(f"è·³è¿‡å·²å­˜åœ¨æ–‡ç« : {aid} -> {existed}")
                                self.result.successful_articles += 1
                                article_progress.update()
                                continue
                    tasks.append((section, article_url))

            self._process_article_tasks(tasks, article_progress)
            article_progress.finish()

            # 6. ä¿å­˜ç»“æœ
            self._save_results()

            # 7. æ ‡è®°å®Œæˆ
            self.result.mark_completed()

            logger.info("="*60)
            logger.info("æŠ“å–å®Œæˆ!")
            logger.info(f"æˆåŠŸæŠ“å–: {self.result.successful_articles}/{self.result.total_articles} ç¯‡æ–‡ç« ")
            logger.info(f"æˆåŠŸç‡: {self.result.get_success_rate():.1%}")
            logger.info(f"è€—æ—¶: {self.result.duration}")
            logger.info("="*60)

            return self.result

        except KeyboardInterrupt:
            logger.warning("ç”¨æˆ·ä¸­æ–­æŠ“å–")
            self._save_results()
            return self.result

    def scrape_all_via_api(self, per_category_limit: Optional[int] = None) -> ScrapingResult:
        """é€šè¿‡ KF5 API åˆ—è¡¨é©±åŠ¨æŠ“å–ï¼ˆæ›´ä¸æ˜“æ¼ï¼‰ã€‚"""
        logger.info("="*60)
        logger.info("å¼€å§‹é€šè¿‡ API åˆ—è¡¨é©±åŠ¨æŠ“å–")
        logger.info("="*60)

        self.result = ScrapingResult()
        self.visited_urls.clear()

        if not self.kf5:
            logger.error("KF5 API æœªé…ç½®æˆ–åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ä½¿ç”¨ API åˆ—è¡¨é©±åŠ¨")
            return self.result

        try:
            # 1. é¦–å…ˆæ„å»ºåˆ†ç±»æ˜ å°„
            logger.info("ğŸ—‚ï¸  æ„å»ºåˆ†ç±»æ˜ å°„...")
            forum_mapping = self.kf5.build_category_mapping()
            logger.info(f"ğŸ“‹ è·å–åˆ° {len(forum_mapping)} ä¸ªåˆ†ç±»æ˜ å°„")

            # å…ˆåˆ†é¡µæ‹‰å–å…¨éƒ¨ posts åˆ—è¡¨ï¼Œä¼˜å…ˆä½¿ç”¨ API æä¾›çš„æ–‡ç«  URL
            page = 1
            per_page = 100
            raw_posts: List[dict] = []  # ä¿ç•™ {id, url, title, forum_id, forum_name}
            while True:
                data = self.kf5.list_all_posts(page=page, per_page=per_page)
                items = data.get('posts') or data.get('data') or data.get('items') or []
                if not isinstance(items, list) or not items:
                    break
                for it in items:
                    aid = str(it.get('id') or it.get('post_id') or it.get('article_id') or '').strip()
                    url = (it.get('url') or '').strip()
                    title = (it.get('title') or '').strip()
                    forum_id = it.get('forum_id')
                    forum_name = it.get('forum_name', '')

                    # å®¹é”™ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨IDæ„é€  KB URLï¼›è‹¥URLå·²ç»™å‡ºä½†éKBä¸”æœ‰IDï¼Œä¹Ÿå›é€€ä¸ºKB URL
                    if not url and aid:
                        url = f"/hc/kb/article/{aid}/"
                    elif url and '/hc/kb/article/' not in url and aid:
                        url = f"/hc/kb/article/{aid}/"
                    # ä»…å½“è‡³å°‘æœ‰ id æˆ– url æ—¶åŠ å…¥
                    if aid or url:
                        raw_posts.append({
                            'id': aid,
                            'url': url,
                            'title': title,
                            'forum_id': forum_id,
                            'forum_name': forum_name
                        })
                if len(items) < per_page:
                    break
                page += 1

            logger.info(f"API è¿”å›å¯èƒ½çš„KBæ–‡ç« : {len(raw_posts)}")

            from .models import Section

            category_counts: Dict[str, int] = {}
            filtered_posts: List[Tuple[dict, str]] = []
            for post in raw_posts:
                forum_id = post.get('forum_id')
                forum_name = post.get('forum_name', '')

                if forum_id and forum_id in forum_mapping:
                    category_path = forum_mapping[forum_id]['full_path']
                    post['forum_name'] = forum_mapping[forum_id]['forum_name']
                elif forum_name:
                    category_path = f"å…¶ä»–/{forum_name}"
                else:
                    category_path = "å…¶ä»–/æœªçŸ¥"

                if per_category_limit is not None:
                    count = category_counts.get(category_path, 0)
                    if count >= per_category_limit:
                        continue
                    category_counts[category_path] = count + 1
                else:
                    category_counts[category_path] = category_counts.get(category_path, 0) + 1

                filtered_posts.append((post, category_path))

            self.result.total_articles = len(filtered_posts)
            self.result.total_sections = len({category_path for _, category_path in filtered_posts})
            logger.info(f"ç­›é€‰åå¾…æŠ“å–æ–‡ç« : {self.result.total_articles}")

            article_progress = make_progress(self.result.total_articles or 1, "æŠ“å–æ–‡ç« :")
            tasks: List[Tuple[Section, str]] = []
            sections_for_categories: List[Section] = []

            for post, category_path in filtered_posts:
                article_url = urljoin(self.base_url, post.get('url') or f"/hc/kb/article/{post.get('id')}/")
                forum_name = post.get('forum_name', 'æœªçŸ¥åˆ†ç±»')

                article_section = Section(
                    url="",
                    title=forum_name,
                    description="",
                    articles=[article_url],
                    category_path=category_path
                )
                article_section.article_count = len(article_section.articles)
                sections_for_categories.append(article_section)

                if self.skip_existing:
                    pid = str(post.get('id') or '').strip()
                    if pid:
                        existed = self._existing_html_for_id(pid)
                        if existed:
                            logger.info(f"è·³è¿‡å·²å­˜åœ¨æ–‡ç« : {pid} -> {existed}")
                            self.result.successful_articles += 1
                            article_progress.update()
                            continue

                tasks.append((article_section, article_url))

            self.result.categories = self._organize_by_categories(sections_for_categories)

            self._process_article_tasks(tasks, article_progress)
            article_progress.finish()

            # ä¿å­˜ç»“æœå¹¶æ ‡è®°
            self._save_results()
            self.result.mark_completed()
            logger.info("æŠ“å–å®Œæˆ(åŸºäºAPIåˆ—è¡¨)")
            logger.info(f"æˆåŠŸæŠ“å–: {self.result.successful_articles}/{self.result.total_articles}")
            return self.result

        except KeyboardInterrupt:
            logger.warning("ç”¨æˆ·ä¸­æ–­æŠ“å–")
            self._save_results()
            return self.result
        except Exception as e:
            logger.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            self._save_results()
            return self.result


    def scrape_categories(self, category_names: List[str]) -> ScrapingResult:
        """åªæŠ“å–æŒ‡å®šåˆ†ç±»çš„æ–‡æ¡£"""
        logger.info(f"å¼€å§‹æŠ“å–æŒ‡å®šåˆ†ç±»: {category_names}")
        
        # è·å–æ‰€æœ‰sectioné“¾æ¥
        section_links = self._extract_section_links()
        
        # è¿‡æ»¤å‡ºæŒ‡å®šåˆ†ç±»çš„sections
        filtered_sections = []
        for section_url in section_links:
            section = self._extract_section_info(section_url)
            if section:
                main_category = section.category_path.split('/')[0]
                if main_category in category_names:
                    filtered_sections.append(section)
        
        logger.info(f"æ‰¾åˆ° {len(filtered_sections)} ä¸ªåŒ¹é…çš„sections")
        
        # ä½¿ç”¨è¿‡æ»¤åçš„sectionsç»§ç»­æŠ“å–
        # ... (ç±»ä¼¼scrape_allçš„é€»è¾‘)
        
        return self.result
    
    def _save_results(self) -> None:
        """ä¿å­˜æŠ“å–ç»“æœ"""
        logger.info("ä¿å­˜æŠ“å–ç»“æœ...")

        # ç”ŸæˆHTMLç´¢å¼•é¡µé¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_images and self.html_generator:
            try:
                index_file = self.html_generator.generate_index_html(
                    self.result.categories,
                    self.result.articles
                )
                logger.info(f"HTMLç´¢å¼•é¡µé¢å·²ç”Ÿæˆ: {index_file}")
            except Exception as e:
                logger.error(f"ç”ŸæˆHTMLç´¢å¼•é¡µé¢å¤±è´¥: {e}")

        # ä¿å­˜å›¾ç‰‡ä¸‹è½½ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_images and self.image_downloader:
            image_stats = self.image_downloader.get_download_stats()
            logger.info(f"å›¾ç‰‡ä¸‹è½½ç»Ÿè®¡: æˆåŠŸ {image_stats['images_downloaded']}, å¤±è´¥ {image_stats['failed']}")
            if image_stats.get('attachments_downloaded', 0) > 0:
                logger.info(f"é™„ä»¶ä¸‹è½½ç»Ÿè®¡: æˆåŠŸ {image_stats['attachments_downloaded']}")

        logger.info("ç»“æœä¿å­˜å®Œæˆ")
    
    def _generate_report(self) -> None:
        """ç”ŸæˆæŠ“å–æŠ¥å‘Š"""
        report_file = self.output_dir / "scraping_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# kintoneå¼€å‘è€…æ–‡æ¡£æŠ“å–æŠ¥å‘Š\n\n")
            f.write(f"**æŠ“å–æ—¶é—´**: {self.result.start_time}\n")
            f.write(f"**å®Œæˆæ—¶é—´**: {self.result.end_time}\n")
            f.write(f"**æ€»è€—æ—¶**: {self.result.duration}\n\n")
            
            f.write("## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ\n\n")
            f.write(f"- **æ€»Sectionæ•°**: {self.result.total_sections}\n")
            f.write(f"- **æ€»æ–‡ç« æ•°**: {self.result.total_articles}\n")
            f.write(f"- **æˆåŠŸæŠ“å–**: {self.result.successful_articles}\n")
            f.write(f"- **å¤±è´¥æ•°é‡**: {self.result.failed_articles}\n")
            f.write(f"- **æˆåŠŸç‡**: {self.result.get_success_rate():.1%}\n\n")
            
            f.write("## ğŸ“‚ åˆ†ç±»ç»Ÿè®¡\n\n")
            for category in self.result.categories:
                f.write(f"### {category.name}\n")
                f.write(f"- **æ€»æ–‡ç« æ•°**: {category.total_articles}\n")
                f.write(f"- **Sections**: {len(category.sections)}\n\n")
                
                for section in category.sections:
                    f.write(f"  - **{section.title}**: {section.article_count} ç¯‡æ–‡ç« \n")
                f.write("\n")
        
        logger.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
