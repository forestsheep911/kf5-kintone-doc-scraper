"""å›¾ç‰‡ä¸‹è½½å™¨æ¨¡å—"""

import hashlib
import logging
import mimetypes
import os
import time
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

from .config import (
    DEFAULT_HEADERS, REQUEST_DELAY, REQUEST_TIMEOUT,
    get_article_file_path, calculate_relative_path, BILIBILI_VIDEO_MODE
)
from .utils import get_safe_filename, rate_limit

logger = logging.getLogger(__name__)


class ImageDownloader:
    """å›¾ç‰‡ä¸‹è½½å™¨"""
    
    def __init__(self, base_url: str, output_dir: Path, try_external_images: bool = False, bilibili_mode: str = None):
        self.base_url = base_url
        self.output_dir = output_dir
        self.try_external_images = try_external_images  # æ˜¯å¦å°è¯•ä¸‹è½½å¤–éƒ¨å›¾ç‰‡
        self.bilibili_mode = bilibili_mode or BILIBILI_VIDEO_MODE  # Bç«™è§†é¢‘å¤„ç†æ¨¡å¼
        self.images_dir = output_dir / "images"
        self.attachments_dir = output_dir / "attachments"
        self.images_dir.mkdir(parents=True, exist_ok=True)
        self.attachments_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºsession
        self.session = requests.Session()
        self.session.headers.update(DEFAULT_HEADERS)

        # è·Ÿè¸ªå·²ä¸‹è½½çš„å›¾ç‰‡å’Œé™„ä»¶
        self.downloaded_images: Dict[str, str] = {}  # URL -> æœ¬åœ°æ–‡ä»¶å
        self.downloaded_attachments: Dict[str, str] = {}  # URL -> æœ¬åœ°æ–‡ä»¶å
        self.failed_downloads: Set[str] = set()
        
        # å¼ºåˆ¶æ¸…ç†ä»»ä½•å¯èƒ½çš„ç¼“å­˜çŠ¶æ€
        self._reset_download_state()

        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        self.supported_formats = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg'}
        
        # æ”¯æŒçš„é™„ä»¶æ ¼å¼
        self.attachment_formats = {'.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', 
                                 '.zip', '.rar', '.7z', '.txt', '.csv', '.json', '.xml'}
    
    def _reset_download_state(self):
        """é‡ç½®ä¸‹è½½çŠ¶æ€ï¼Œæ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        self.downloaded_images.clear()
        self.downloaded_attachments.clear()
        self.failed_downloads.clear()
        logger.debug("ä¸‹è½½çŠ¶æ€å·²é‡ç½®")
    
    def _get_image_extension(self, url: str, content_type: str = None, content_preview: bytes = None) -> str:
        """è·å–å›¾ç‰‡æ–‡ä»¶æ‰©å±•å"""
        # é¦–å…ˆå°è¯•ä»URLè·å–æ‰©å±•å
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()
        
        for ext in self.supported_formats:
            if path.endswith(ext):
                return ext
        
        # å°è¯•ä»æ–‡ä»¶å†…å®¹çš„é­”æœ¯å­—èŠ‚åˆ¤æ–­æ ¼å¼
        if content_preview:
            if content_preview.startswith(b'\xFF\xD8\xFF'):
                return '.jpg'
            elif content_preview.startswith(b'\x89PNG\r\n\x1a\n'):
                return '.png'
            elif content_preview.startswith(b'GIF8'):
                return '.gif'
            elif content_preview.startswith(b'\x42\x4D'):
                return '.bmp'
            elif content_preview.startswith(b'RIFF') and b'WEBP' in content_preview[:20]:
                return '.webp'
        
        # å¦‚æœURLæ²¡æœ‰æ‰©å±•åï¼Œå°è¯•ä»Content-Typeè·å–
        if content_type:
            ext = mimetypes.guess_extension(content_type.split(';')[0])
            if ext and ext.lower() in self.supported_formats:
                return ext.lower()
        
        # é»˜è®¤ä½¿ç”¨.jpg
        return '.jpg'
    
    def _generate_filename(self, url: str, content_type: str = None, content_preview: bytes = None) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # ä½¿ç”¨URLçš„hashä½œä¸ºæ–‡ä»¶åï¼Œé¿å…é‡å¤å’Œç‰¹æ®Šå­—ç¬¦é—®é¢˜
        url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()[:12]
        
        # è·å–æ‰©å±•å
        extension = self._get_image_extension(url, content_type, content_preview)
        
        return f"{url_hash}{extension}"
    
    def _is_external_image_host(self, url: str) -> bool:
        """æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦æ¥è‡ªå¤–éƒ¨å›¾åºŠ"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            # å·²çŸ¥çš„å¤–éƒ¨å›¾åºŠåŸŸå
            external_hosts = {
                's3.bmp.ovh',
                'imgchr.com',
                'imgtu.com',
                'sm.ms',
                'imgur.com',
                'githubusercontent.com',
                'raw.githubusercontent.com',
                'cloudflare-ipfs.com',
                'ipfs.io',
                'pinata.cloud',
                'arweave.net',
                'nft.storage',
                'web3.storage',
                'infura-ipfs.io'
            }

            # æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥çš„å¤–éƒ¨å›¾åºŠ
            if any(host in domain for host in external_hosts):
                return True

            # æ£€æŸ¥æ˜¯å¦æ˜¯å½“å‰ç½‘ç«™çš„å­åŸŸåä»¥å¤–çš„å…¶ä»–åŸŸå
            base_domain = urlparse(self.base_url).netloc.lower()
            if parsed.netloc and parsed.netloc != base_domain:
                # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸»åŸŸåçš„ä¸åŒå­åŸŸå
                # ä¾‹å¦‚ï¼šfiles.kf5.com å’Œ cybozudev.kf5.com éƒ½å±äº kf5.com
                def get_main_domain(domain):
                    parts = domain.split('.')
                    if len(parts) >= 2:
                        return '.'.join(parts[-2:])  # å–æœ€åä¸¤éƒ¨åˆ†ä½œä¸ºä¸»åŸŸå
                    return domain
                
                main_domain = get_main_domain(base_domain)
                image_main_domain = get_main_domain(domain)
                
                # å¦‚æœä¸æ˜¯åŒä¸€ä¸»åŸŸåï¼Œåˆ™è®¤ä¸ºæ˜¯å¤–éƒ¨å›¾åºŠ
                if image_main_domain != main_domain:
                    return True

            return False
        except Exception:
            return False

    def _is_valid_image_url(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡URL"""
        if not url:
            return False

        # æ£€æŸ¥URLæ ¼å¼
        try:
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return False
        except Exception:
            return False

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        path = parsed.path.lower()
        if any(path.endswith(ext) for ext in self.supported_formats):
            return True

        # å¯¹äºæ²¡æœ‰æ˜ç¡®æ‰©å±•åçš„URLï¼Œä¹Ÿå°è¯•ä¸‹è½½
        return True
    
    def download_image(self, image_url: str) -> Optional[str]:
        """ä¸‹è½½å•ä¸ªå›¾ç‰‡ï¼Œè¿”å›æœ¬åœ°æ–‡ä»¶å"""
        if not self._is_valid_image_url(image_url):
            logger.debug(f"è·³è¿‡æ— æ•ˆå›¾ç‰‡URL: {image_url}")
            return None

        # è½¬æ¢ä¸ºç»å¯¹URL
        absolute_url = urljoin(self.base_url, image_url)

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¤–éƒ¨å›¾åºŠ
        is_external = self._is_external_image_host(absolute_url)
        if is_external and not self.try_external_images:
            logger.debug(f"è·³è¿‡å¤–éƒ¨å›¾åºŠå›¾ç‰‡: {absolute_url}")
            return None
        elif is_external:
            logger.info(f"å°è¯•ä¸‹è½½å¤–éƒ¨å›¾åºŠå›¾ç‰‡: {absolute_url}")
        else:
            logger.debug(f"ä¸‹è½½åŒåŸŸå›¾ç‰‡: {absolute_url}")

        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½è¿‡
        if absolute_url in self.downloaded_images:
            cached_filename = self.downloaded_images[absolute_url]
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„å›¾ç‰‡: {absolute_url} -> {cached_filename}")
            return cached_filename

        # æ£€æŸ¥æ˜¯å¦ä¸‹è½½å¤±è´¥è¿‡
        if absolute_url in self.failed_downloads:
            logger.debug(f"è·³è¿‡å·²çŸ¥å¤±è´¥çš„å›¾ç‰‡: {absolute_url}")
            return None
        
        # å°è¯•ä¸‹è½½å›¾ç‰‡ï¼ˆä¸é‡è¯•ï¼Œé¿å…æµªè´¹æ—¶é—´ï¼‰
        max_retries = 0
        for attempt in range(max_retries + 1):
            try:
                logger.info(f"ä¸‹è½½å›¾ç‰‡: {absolute_url}")
                
                # ä¸ºå¤–éƒ¨å›¾åºŠè®¾ç½®æ›´å¥½çš„è¯·æ±‚å¤´
                if is_external:
                    logger.debug(f"æ£€æµ‹åˆ°å¤–éƒ¨å›¾åºŠ: {absolute_url}")
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'DNT': '1',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                        'Sec-Fetch-Dest': 'image',
                        'Sec-Fetch-Mode': 'no-cors',
                        'Sec-Fetch-Site': 'cross-site',
                        'Cache-Control': 'no-cache',
                        'Pragma': 'no-cache',
                    }
                    
                    # åˆ›å»ºæ–°çš„sessionæ¥é¿å…cookieå¹²æ‰°
                    external_session = requests.Session()
                    external_session.headers.update(headers)
                    
                    # å¯¹äºæŸäº›ç‰¹æ®ŠåŸŸåï¼Œè°ƒæ•´è¯·æ±‚å¤´
                    if 's3.bmp.ovh' in absolute_url:
                        headers['Referer'] = 'https://bmp.ovh/'
                        headers['Origin'] = 'https://bmp.ovh'
                        external_session.headers.update(headers)
                        logger.debug(f"ä¸ºs3.bmp.ovhè®¾ç½®ç‰¹æ®Šè¯·æ±‚å¤´")
                    
                    response = external_session.get(absolute_url, timeout=REQUEST_TIMEOUT, stream=True, allow_redirects=True)
                    logger.debug(f"å¤–éƒ¨å›¾ç‰‡è¯·æ±‚å®Œæˆï¼ŒçŠ¶æ€ç : {response.status_code}")
                else:
                    # ä½¿ç”¨sessionä¸‹è½½åŒåŸŸå›¾ç‰‡
                    response = self.session.get(absolute_url, timeout=REQUEST_TIMEOUT, stream=True)
                    logger.debug(f"å†…éƒ¨å›¾ç‰‡è¯·æ±‚å®Œæˆï¼ŒçŠ¶æ€ç : {response.status_code}")
                
                response.raise_for_status()
                logger.debug(f"å›¾ç‰‡è¯·æ±‚æˆåŠŸ: {absolute_url}")
                
                # æ£€æŸ¥Content-Typeå’ŒURLæ‰©å±•å
                content_type = response.headers.get('content-type', '')
                
                # ä»URLè·å–å¯èƒ½çš„æ‰©å±•å
                parsed_url = urlparse(absolute_url)
                url_path = parsed_url.path.lower()
                has_image_extension = any(url_path.endswith(ext) for ext in self.supported_formats)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡ï¼šContent-Typeæ˜¯image/*ï¼Œæˆ–è€…URLæœ‰å›¾ç‰‡æ‰©å±•åï¼Œæˆ–è€…æ˜¯kf5.comçš„é™„ä»¶é“¾æ¥ï¼Œæˆ–è€…æ˜¯å¤–éƒ¨å›¾åºŠ
                is_image_content_type = content_type.startswith('image/')
                is_kf5_attachment = 'files.kf5.com' in absolute_url or 'attachments/download' in absolute_url
                
                # å¯¹äºå¤–éƒ¨å›¾åºŠï¼Œæ›´å®½æ¾çš„éªŒè¯ - åªè¦URLçœ‹èµ·æ¥åƒå›¾ç‰‡å°±å°è¯•ä¸‹è½½
                if not (is_image_content_type or has_image_extension or is_kf5_attachment or is_external):
                    logger.warning(f"URLä¸æ˜¯å›¾ç‰‡: {absolute_url} (Content-Type: {content_type}, è·¯å¾„: {url_path})")
                    self.failed_downloads.add(absolute_url)
                    return None
                
                # å¦‚æœæ˜¯å¤–éƒ¨å›¾åºŠä½†Content-Typeä¸æ˜¯image/*ï¼Œè®°å½•ä½†ç»§ç»­å°è¯•
                if is_external and not is_image_content_type:
                    logger.debug(f"å¤–éƒ¨å›¾åºŠContent-Typeä¸æ˜¯image/*ï¼Œä½†ä»å°è¯•ä¸‹è½½: {absolute_url} (Content-Type: {content_type})")
                
                # å¯¹äºkf5.comçš„æ–‡ä»¶ï¼Œå³ä½¿Content-Typeä¸æ˜¯image/*ï¼Œä¹Ÿå°è¯•ä¸‹è½½
                if is_kf5_attachment and not is_image_content_type:
                    logger.info(f"å°è¯•ä¸‹è½½kf5.comé™„ä»¶ä½œä¸ºå›¾ç‰‡: {absolute_url} (Content-Type: {content_type})")
                
                # è¯»å–å‰å‡ ä¸ªå­—èŠ‚æ¥åˆ¤æ–­æ–‡ä»¶ç±»å‹
                content_preview = None
                first_chunk = None
                content_chunks = []
                
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        content_chunks.append(chunk)
                        if first_chunk is None:
                            first_chunk = chunk
                            content_preview = chunk[:20]  # è¯»å–å‰20å­—èŠ‚ç”¨äºæ ¼å¼åˆ¤æ–­
                        
                            # å¦‚æœä¸æ˜¯æ˜ç¡®çš„å›¾ç‰‡Content-Typeï¼Œé€šè¿‡æ–‡ä»¶å¤´åˆ¤æ–­æ˜¯å¦æ˜¯å›¾ç‰‡
                            if not is_image_content_type:
                                is_image_by_header = (content_preview.startswith(b'\xFF\xD8\xFF') or  # JPEG
                                                    content_preview.startswith(b'\x89PNG\r\n\x1a\n') or  # PNG
                                                    content_preview.startswith(b'GIF8') or  # GIF
                                                    content_preview.startswith(b'\x42\x4D') or  # BMP
                                                    (content_preview.startswith(b'RIFF') and b'WEBP' in content_preview))  # WEBP
                                
                                if not is_image_by_header:
                                    # å¯¹äºå¤–éƒ¨å›¾åºŠï¼Œå³ä½¿æ–‡ä»¶å¤´ä¸åŒ¹é…ä¹Ÿå°è¯•ä¿å­˜ï¼ˆå¯èƒ½æ˜¯ç‰¹æ®Šæ ¼å¼æˆ–å‹ç¼©ï¼‰
                                    if is_external:
                                        logger.warning(f"å¤–éƒ¨å›¾åºŠæ–‡ä»¶å¤´ä¸åŒ¹é…ï¼Œä½†ä»å°è¯•ä¿å­˜: {absolute_url} (æ–‡ä»¶å¤´: {content_preview[:10].hex()})")
                                    else:
                                        logger.warning(f"æ–‡ä»¶ä¸æ˜¯å›¾ç‰‡æ ¼å¼: {absolute_url} (æ–‡ä»¶å¤´: {content_preview[:10].hex()})")
                                        self.failed_downloads.add(absolute_url)
                                        return None
                                else:
                                    logger.info(f"é€šè¿‡æ–‡ä»¶å¤´ç¡®è®¤ä¸ºå›¾ç‰‡: {absolute_url}")
                
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨å†…å®¹é¢„è§ˆæ¥æ›´å‡†ç¡®åœ°åˆ¤æ–­æ‰©å±•åï¼‰
                filename = self._generate_filename(absolute_url, content_type, content_preview)
                filepath = self.images_dir / filename
                
                # ä¿å­˜å›¾ç‰‡
                with open(filepath, 'wb') as f:
                    for chunk in content_chunks:
                        f.write(chunk)
                
                logger.debug(f"å›¾ç‰‡ä¿å­˜æˆåŠŸ: {filename}")
                self.downloaded_images[absolute_url] = filename
                
                # æ§åˆ¶ä¸‹è½½é€Ÿåº¦
                rate_limit(REQUEST_DELAY * 0.5)  # å›¾ç‰‡ä¸‹è½½ç¨å¿«ä¸€äº›
                
                return filename
                
            except (requests.RequestException, Exception) as e:
                # ä¸é‡è¯•ï¼Œç›´æ¥è®°å½•å¤±è´¥
                logger.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {absolute_url} - {str(e)}")
                self.failed_downloads.add(absolute_url)
                return None

    def _extract_github_url_from_license(self, license_url: str, link_text: str) -> Optional[str]:
        """ä»licenseæ–‡ä»¶é“¾æ¥ä¸­æå–GitHubé¡¹ç›®URL"""
        try:
            # å¸¸è§çš„GitHubé¡¹ç›®åç§°æ¨¡å¼
            import re
            
            # ä»é“¾æ¥æ–‡æœ¬ä¸­æå–å¯èƒ½çš„é¡¹ç›®åç§°
            # ä¾‹å¦‚: "MIT-LICENSE_115.txt" -> å¯èƒ½æ˜¯æŸä¸ªé¡¹ç›®çš„license
            # æˆ–è€…é“¾æ¥æ–‡æœ¬æœ¬èº«å°±åŒ…å«é¡¹ç›®åç§°
            
            # ä¸€äº›å·²çŸ¥çš„é¡¹ç›®æ˜ å°„ï¼ˆå¯ä»¥æ ¹æ®å®é™…æƒ…å†µæ‰©å±•ï¼‰
            project_mappings = {
                'express': 'https://github.com/expressjs/express',
                'react': 'https://github.com/facebook/react',
                'vue': 'https://github.com/vuejs/vue',
                'angular': 'https://github.com/angular/angular',
                'jquery': 'https://github.com/jquery/jquery',
                'bootstrap': 'https://github.com/twbs/bootstrap',
                'lodash': 'https://github.com/lodash/lodash',
                'moment': 'https://github.com/moment/moment',
                'axios': 'https://github.com/axios/axios',
                'webpack': 'https://github.com/webpack/webpack',
                'babel': 'https://github.com/babel/babel',
                'eslint': 'https://github.com/eslint/eslint',
                'typescript': 'https://github.com/microsoft/TypeScript',
                'node': 'https://github.com/nodejs/node',
                'npm': 'https://github.com/npm/npm',
            }
            
            # æ£€æŸ¥é“¾æ¥æ–‡æœ¬æˆ–URLä¸­æ˜¯å¦åŒ…å«å·²çŸ¥é¡¹ç›®åç§°
            text_lower = link_text.lower()
            url_lower = license_url.lower()
            
            for project_name, github_url in project_mappings.items():
                if project_name in text_lower or project_name in url_lower:
                    logger.debug(f"æ‰¾åˆ°åŒ¹é…çš„é¡¹ç›®: {project_name} -> {github_url}")
                    return github_url
            
            # å¦‚æœæ— æ³•åŒ¹é…åˆ°å…·ä½“é¡¹ç›®ï¼Œè¿”å›None
            logger.debug(f"æ— æ³•ç¡®å®šlicenseæ–‡ä»¶å¯¹åº”çš„GitHubé¡¹ç›®: {license_url}")
            return None
            
        except Exception as e:
            logger.warning(f"æå–GitHub URLæ—¶å‡ºé”™: {e}")
            return None

    def download_attachment(self, attachment_url: str) -> Optional[str]:
        """ä¸‹è½½é™„ä»¶ï¼Œè¿”å›æœ¬åœ°æ–‡ä»¶å"""
        if not attachment_url:
            return None

        # è½¬æ¢ä¸ºç»å¯¹URL
        absolute_url = urljoin(self.base_url, attachment_url)
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½è¿‡
        if absolute_url in self.downloaded_attachments:
            return self.downloaded_attachments[absolute_url]

        # æ£€æŸ¥æ˜¯å¦ä¸‹è½½å¤±è´¥è¿‡
        if absolute_url in self.failed_downloads:
            return None
        
        try:
            logger.info(f"ä¸‹è½½é™„ä»¶: {absolute_url}")
            
            # ä¸‹è½½é™„ä»¶
            response = self.session.get(absolute_url, timeout=REQUEST_TIMEOUT, stream=True)
            response.raise_for_status()
            
            # ç”ŸæˆåŸºäºURLçš„å”¯ä¸€æ–‡ä»¶åï¼Œé¿å…é‡å¤ä¸‹è½½ç›¸åŒæ–‡ä»¶
            import hashlib
            url_hash = hashlib.md5(absolute_url.encode()).hexdigest()[:8]
            
            # å°è¯•ä»å“åº”å¤´è·å–åŸå§‹æ–‡ä»¶å
            content_disposition = response.headers.get('content-disposition', '')
            original_filename = None
            
            if content_disposition:
                import re
                # æ”¯æŒRFC 5987æ ¼å¼ï¼šfilename*=UTF-8''filename
                rfc5987_match = re.search(r"filename\*=UTF-8''([^;]+)", content_disposition)
                if rfc5987_match:
                    from urllib.parse import unquote
                    original_filename = unquote(rfc5987_match.group(1))
                    logger.debug(f"ä»RFC5987æ ¼å¼è§£ææ–‡ä»¶å: {original_filename}")
                else:
                    # ä¼ ç»Ÿæ ¼å¼ï¼šfilename="filename" æˆ– filename=filename
                    traditional_match = re.search(r'filename[*]?=([^;]+)', content_disposition)
                    if traditional_match:
                        original_filename = traditional_match.group(1).strip('"\'')
                        logger.debug(f"ä»ä¼ ç»Ÿæ ¼å¼è§£ææ–‡ä»¶å: {original_filename}")
            
            # å¦‚æœæ— æ³•ä»å“åº”å¤´è·å–æ–‡ä»¶åï¼Œä»URLè§£æ
            if not original_filename:
                from urllib.parse import urlparse, unquote
                parsed_url = urlparse(absolute_url)
                path_parts = parsed_url.path.split('/')
                
                # å¯»æ‰¾å¯èƒ½çš„æ–‡ä»¶å
                for part in reversed(path_parts):
                    if part and '.' in part:
                        original_filename = unquote(part)
                        logger.debug(f"ä»URLè·¯å¾„è§£ææ–‡ä»¶å: {original_filename}")
                        break
            
            # ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶åï¼šhash_åŸå§‹åç§° æˆ– hash.æ‰©å±•å
            if original_filename:
                # æ¸…ç†åŸå§‹æ–‡ä»¶å
                clean_original = get_safe_filename(original_filename, max_length=50)
                filename = f"{url_hash}_{clean_original}"
            else:
                # æ ¹æ®Content-Typeç”Ÿæˆæ‰©å±•å
                content_type = response.headers.get('content-type', '').lower()
                
                if 'pdf' in content_type:
                    filename = f"{url_hash}.pdf"
                elif 'zip' in content_type or 'compressed' in content_type:
                    filename = f"{url_hash}.zip"
                elif 'word' in content_type or 'msword' in content_type:
                    filename = f"{url_hash}.doc"
                elif 'excel' in content_type or 'spreadsheet' in content_type:
                    filename = f"{url_hash}.xlsx"
                elif 'json' in content_type:
                    filename = f"{url_hash}.json"
                elif 'text' in content_type:
                    filename = f"{url_hash}.txt"
                else:
                    filename = f"{url_hash}.bin"
                    
                logger.debug(f"æ ¹æ®Content-Typeç”Ÿæˆæ–‡ä»¶å: {filename} (Content-Type: {content_type})")
            
            # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
            filename = get_safe_filename(filename)
            filepath = self.attachments_dir / filename
            
            # é¿å…æ–‡ä»¶åå†²çª
            counter = 1
            original_filepath = filepath
            while filepath.exists():
                name_part = original_filepath.stem
                ext_part = original_filepath.suffix
                filepath = self.attachments_dir / f"{name_part}_{counter}{ext_part}"
                counter += 1
            
            # ä¿å­˜é™„ä»¶
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            logger.debug(f"é™„ä»¶ä¿å­˜æˆåŠŸ: {filepath.name}")
            self.downloaded_attachments[absolute_url] = filepath.name
            
            # æ§åˆ¶ä¸‹è½½é€Ÿåº¦
            rate_limit(REQUEST_DELAY)
            
            return filepath.name
            
        except requests.RequestException as e:
            logger.error(f"ä¸‹è½½é™„ä»¶å¤±è´¥ {absolute_url}: {e}")
            self.failed_downloads.add(absolute_url)
            return None
        except Exception as e:
            logger.error(f"ä¿å­˜é™„ä»¶å¤±è´¥ {absolute_url}: {e}")
            self.failed_downloads.add(absolute_url)
            return None
    
    def process_html_images(self, html_content: str, article_title: str = "", article_url: str = "", article_category: str = "", current_section_category: str = "") -> Tuple[str, List[str]]:
        """
        å¤„ç†HTMLä¸­çš„å›¾ç‰‡å’Œé“¾æ¥
        - ä¸‹è½½å›¾ç‰‡å¹¶æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
        - å°†æœ‰æ•ˆè¶…é“¾æ¥è½¬æ¢ä¸ºspanæ ‡ç­¾ï¼Œä¿ç•™åŸå§‹ä¿¡æ¯
        - å°†åŒä¸€ç½‘ç«™çš„æ–‡ç« é“¾æ¥è½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶é“¾æ¥
        - å¯¹äºæ— æ•ˆé“¾æ¥ï¼ˆå¦‚javascript:;ï¼‰ï¼Œç›´æ¥ç§»é™¤é“¾æ¥ä¿ç•™çº¯æ–‡æœ¬
        è¿”å›: (æ›´æ–°åçš„HTML, ä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶ååˆ—è¡¨)
        """
        if not html_content:
            return html_content, []
        
        soup = BeautifulSoup(html_content, 'html.parser')
        downloaded_files = []

        # ç”±äºHTMLä½¿ç”¨äº†baseæ ‡ç­¾æŒ‡å‘outputæ ¹ç›®å½•ï¼Œ
        # å›¾ç‰‡è·¯å¾„åº”è¯¥ç›´æ¥åŸºäºoutputç›®å½•ï¼Œä¸éœ€è¦../å‰ç¼€
        images_relative_path = "images"

        # è¾…åŠ©å‡½æ•°ï¼šå°†åŒç«™ç‚¹æ–‡ç« é“¾æ¥è½¬æ¢ä¸ºæœ¬åœ°é“¾æ¥
        def convert_to_local_link(href: str) -> Optional[tuple]:
            """å°†åŒä¸€ç½‘ç«™çš„æ–‡ç« é“¾æ¥è½¬æ¢ä¸ºæ–‡ç« IDå’Œé“¾æ¥ç±»å‹
            
            Returns:
                tuple: (link_type, article_id_or_anchor)
                - ('anchor', anchor): é¡µé¢å†…é”šç‚¹
                - ('article', article_id): å¤–éƒ¨æ–‡ç« é“¾æ¥
                - None: æ— æ•ˆé“¾æ¥
            """
            logger.debug(f"æ£€æŸ¥é“¾æ¥è½¬æ¢: {href} (å½“å‰æ–‡ç« : {article_url})")

            if not href or not article_url:
                logger.debug("é“¾æ¥æˆ–æ–‡ç« URLä¸ºç©ºï¼Œè·³è¿‡è½¬æ¢")
                return None

            # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ç½‘ç«™çš„é“¾æ¥ï¼ˆåŒ…æ‹¬ç›¸å¯¹è·¯å¾„ï¼‰
            is_same_site = (
                'cybozudev.kf5.com' in href or  # å®Œæ•´URL
                href.startswith('/hc/kb/article/') or  # ç›¸å¯¹è·¯å¾„çš„æ–‡ç« é“¾æ¥
                href.startswith('../') or  # ç›¸å¯¹è·¯å¾„
                (href.startswith('/') and 'hc/kb' in href)  # å…¶ä»–ç›¸å¯¹è·¯å¾„
            )
            
            if not is_same_site:
                logger.debug(f"ä¸æ˜¯åŒä¸€ç½‘ç«™çš„é“¾æ¥ï¼Œè·³è¿‡è½¬æ¢: {href}")
                return None

            # æå–æ–‡ç« ID
            import re
            article_match = re.search(r'/hc/kb/article/(\d+)', href)
            if not article_match:
                logger.debug(f"æœªæ‰¾åˆ°æ–‡ç« IDï¼Œè·³è¿‡è½¬æ¢: {href}")
                return None

            target_article_id = article_match.group(1)
            logger.debug(f"æå–åˆ°æ–‡ç« ID: {target_article_id}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯å½“å‰æ–‡ç« çš„é”šç‚¹é“¾æ¥
            if f'/hc/kb/article/{target_article_id}' in article_url:
                # è¿™æ˜¯æŒ‡å‘å½“å‰æ–‡ç« çš„é“¾æ¥
                if '#' in href:
                    # å½“å‰æ–‡ç« çš„é”šç‚¹é“¾æ¥ï¼Œè¿”å›é”šç‚¹ï¼ˆç¨åä¼šè½¬æ¢ä¸ºç²—ä½“ï¼‰
                    anchor_match = re.search(r'#(.+)$', href)
                    if anchor_match:
                        logger.debug(f"è½¬æ¢ä¸ºå†…éƒ¨é”šç‚¹: #{anchor_match.group(1)}")
                        return ('anchor', anchor_match.group(1))
                # å½“å‰æ–‡ç« çš„é“¾æ¥ä½†æ²¡æœ‰é”šç‚¹ï¼Œå¿½ç•¥
                logger.debug(f"å¿½ç•¥æŒ‡å‘å½“å‰æ–‡ç« çš„é“¾æ¥: {href}")
                return None

            # è¿”å›å¤–éƒ¨æ–‡ç« ID
            logger.debug(f"è½¬æ¢ä¸ºå¤–éƒ¨æ–‡ç« é“¾æ¥: {href} -> æ–‡ç« ID: {target_article_id}")
            return ('article', target_article_id)

        
        # 0. å¤„ç†iframeï¼ˆç‰¹åˆ«æ˜¯Bç«™è§†é¢‘ï¼‰
        iframes = soup.find_all('iframe')
        for iframe in iframes:
            src = iframe.get('src', '')
            if 'bilibili.com' in src or 'player.bilibili.com' in src:
                # æå–è§†é¢‘ä¿¡æ¯
                import re
                bv_match = re.search(r'bvid=([^&]+)', src)
                aid_match = re.search(r'aid=(\d+)', src)
                
                if bv_match or aid_match:
                    # æ ¹æ®é…ç½®æ¨¡å¼å¤„ç†Bç«™è§†é¢‘
                    if bv_match:
                        video_id = bv_match.group(1)
                        aid = aid_match.group(1) if aid_match else None
                        video_info = video_id
                    else:
                        aid = aid_match.group(1)
                        video_id = None
                        video_info = f"av{aid}"
                    
                    # æ„å»ºè§†é¢‘URL
                    if video_id:
                        video_url = f"https://www.bilibili.com/video/{video_id}"
                    else:
                        video_url = f"https://www.bilibili.com/video/av{aid}"
                    
                    # åˆ›å»ºå‹å¥½çš„Bç«™è§†é¢‘é“¾æ¥
                    container = soup.new_tag('div')
                    container['class'] = 'bilibili-video-link'
                    container['style'] = 'margin: 16px 0; padding: 16px; border: 2px solid #00a1d6; border-radius: 8px; background-color: #f0f8ff; text-align: center;'
                    
                    # è§†é¢‘å›¾æ ‡å’Œæ ‡é¢˜
                    title_p = soup.new_tag('p')
                    title_p['style'] = 'margin: 0 0 8px 0; font-size: 16px; font-weight: bold; color: #333;'
                    title_p.string = f"ğŸ“º Bç«™è§†é¢‘: {video_info}"
                    
                    # æç¤ºæ–‡å­—å’Œé“¾æ¥
                    link_p = soup.new_tag('p')
                    link_p['style'] = 'margin: 8px 0 0 0; font-size: 14px; color: #666;'
                    
                    hint_text = soup.new_tag('span')
                    hint_text.string = "è¯·å‰å¾€Bç«™è§‚çœ‹ï¼š  "
                    
                    a_tag = soup.new_tag('a')
                    a_tag.string = f"ç‚¹å‡»è§‚çœ‹ {video_info} â†’"
                    a_tag['href'] = video_url
                    a_tag['target'] = "_blank"
                    a_tag['rel'] = "noopener noreferrer"
                    a_tag['class'] = "bilibili-link"
                    a_tag['style'] = "color: #00a1d6; text-decoration: none; font-weight: bold; font-size: 15px; padding: 4px 8px; border-radius: 4px; background-color: rgba(0, 161, 214, 0.1);"
                    
                    link_p.append(hint_text)
                    link_p.append(a_tag)
                    
                    container.append(title_p)
                    container.append(link_p)
                    
                    iframe.replace_with(container)
                    logger.debug(f"æ›¿æ¢Bç«™iframeä¸ºå‹å¥½é“¾æ¥: {video_info} -> {video_url}")

        # 1. å¤„ç†å›¾ç‰‡
        img_tags = soup.find_all('img')
        
        if img_tags:
            # ç»Ÿè®¡ä¸åŒçš„å›¾ç‰‡URL
            unique_urls = set()
            for img in img_tags:
                src = img.get('src')
                if src:
                    unique_urls.add(src)
            
            logger.info(f"æ–‡ç«  '{article_title}' ä¸­å‘ç° {len(img_tags)} ä¸ªimgæ ‡ç­¾ï¼Œ{len(unique_urls)} ä¸ªä¸åŒå›¾ç‰‡")
            
            for img in img_tags:
                src = img.get('src')
                if not src:
                    continue

                # ä¸‹è½½å›¾ç‰‡
                filename = self.download_image(src)
                logger.debug(f"å›¾ç‰‡ä¸‹è½½ç»“æœ: {src} -> {filename}")
                
                # å¦‚æœä¸‹è½½å¤±è´¥ä½†å›¾ç‰‡å·²åœ¨ç¼“å­˜ä¸­ï¼Œä½¿ç”¨ç¼“å­˜çš„æ–‡ä»¶åï¼ˆå®¹é”™å¤„ç†ï¼‰
                if not filename:
                    absolute_url = urljoin(self.base_url, src)
                    if absolute_url in self.downloaded_images:
                        filename = self.downloaded_images[absolute_url]
                        logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„å›¾ç‰‡æ–‡ä»¶å: {src} -> {filename}")

                if filename:
                    # æ›´æ–°imgæ ‡ç­¾çš„srcå±æ€§ä¸ºç›¸å¯¹è·¯å¾„
                    img['src'] = f"{images_relative_path}/{filename}"
                    # åªåœ¨é¦–æ¬¡æˆåŠŸä¸‹è½½æ—¶è®°å½•
                    if filename not in downloaded_files:
                        downloaded_files.append(filename)
                    logger.debug(f"å›¾ç‰‡é“¾æ¥å·²æ›´æ–°: {src} -> {images_relative_path}/{filename}")
                else:
                    # å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Œæ›¿æ¢ä¸ºæ–‡æœ¬æç¤º
                    absolute_url = urljoin(self.base_url, src)
                    alt_text = img.get('alt', 'å›¾ç‰‡')
                    
                    logger.warning(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {src}")
                    # åˆ›å»ºå¤±è´¥æç¤ºï¼ŒåŒºåˆ†å¤–éƒ¨å’Œå†…éƒ¨å›¾ç‰‡
                    if self._is_external_image_host(absolute_url):
                        placeholder = soup.new_tag('div')
                        placeholder['class'] = 'external-image-placeholder'
                        placeholder['style'] = 'border: 2px dashed #ffa500; padding: 20px; text-align: center; color: #ff8c00; background-color: #fff8e1; border-radius: 4px; margin: 10px 0;'
                        placeholder.string = f"ğŸŒ å¤–éƒ¨å›¾ç‰‡ä¸‹è½½å¤±è´¥: {alt_text}"
                    else:
                        placeholder = soup.new_tag('div')
                        placeholder['class'] = 'failed-image-placeholder'
                        placeholder['style'] = 'border: 2px dashed #ff6b6b; padding: 20px; text-align: center; color: #c92a2a; background-color: #ffe0e0; border-radius: 4px; margin: 10px 0;'
                        placeholder.string = f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {alt_text}"
                    
                    # æ›¿æ¢imgæ ‡ç­¾
                    img.replace_with(placeholder)
        else:
            logger.debug(f"æ–‡ç«  '{article_title}' ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
        
        # 2. å¤„ç†è¶…é“¾æ¥ - è½¬æ¢ä¸ºspanæ ‡ç­¾æˆ–ç›´æ¥ç§»é™¤æ— æ•ˆé“¾æ¥
        link_tags = soup.find_all('a')
        if link_tags:
            logger.info(f"æ–‡ç«  '{article_title}' ä¸­å‘ç° {len(link_tags)} ä¸ªé“¾æ¥ï¼Œè¿›è¡Œå¤„ç†")

            for link in link_tags:
                href = link.get('href', '').strip()
                link_text = link.get_text(strip=True)

                if not link_text:
                    # å¯¹äºæ²¡æœ‰æ–‡æœ¬çš„é“¾æ¥ï¼Œç›´æ¥ç§»é™¤
                    link.decompose()
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯æ— æ•ˆé“¾æ¥
                invalid_hrefs = [
                    'javascript:;',
                    'javascript:void(0)',
                    'javascript:void(0);',
                    '#',
                    '',
                    None
                ]

                is_invalid_link = (
                    href.lower() in invalid_hrefs or
                    href.startswith('javascript:') or
                    href == '#'
                )

                if is_invalid_link:
                    # å¯¹äºæ— æ•ˆé“¾æ¥ï¼Œè½¬æ¢ä¸ºspanå…ƒç´ ä»¥ä¿æŒæ ·å¼å’Œé—´è·
                    span = soup.new_tag('span')
                    span.string = link_text
                    
                    # å¤åˆ¶åŸæœ‰çš„classå±æ€§ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    if link.get('class'):
                        span['class'] = link.get('class')
                    
                    # æ·»åŠ ä¸€ä¸ªæ ‡è¯†class
                    existing_classes = span.get('class', [])
                    if isinstance(existing_classes, str):
                        existing_classes = [existing_classes]
                    existing_classes.append('inactive-link')
                    span['class'] = existing_classes
                    
                    # æ›¿æ¢åŸæ¥çš„aæ ‡ç­¾
                    link.replace_with(span)
                    logger.debug(f"å°†æ— æ•ˆé“¾æ¥ '{href}' è½¬æ¢ä¸ºspan: {link_text}")
                else:
                    # å°è¯•è½¬æ¢ä¸ºæœ¬åœ°é“¾æ¥
                    link_result = convert_to_local_link(href)

                    if link_result:
                        link_type, link_data = link_result
                        
                        if link_type == 'anchor':
                            # é¡µé¢å†…é”šç‚¹é“¾æ¥ - ä¿ç•™åŸæ ·ä»¥æ”¯æŒé¡µé¢å†…å¯¼èˆª
                            a_tag = soup.new_tag('a')
                            a_tag.string = link_text
                            a_tag['href'] = link_data  # link_dataæ˜¯é”šç‚¹ID
                        elif link_type == 'article':
                            # åŒç«™ç‚¹æ–‡ç« é“¾æ¥ï¼Œä½¿ç”¨æ–‡ç« IDå¼•ç”¨æ ¼å¼ï¼ˆé¿å…ç ´åSPAæ ·å¼ï¼‰
                            a_tag = soup.new_tag('a')
                            a_tag.string = link_text
                            a_tag['href'] = '#'  # ä¸ç›´æ¥è·³è½¬
                            a_tag['data-article-id'] = link_data  # link_dataæ˜¯æ–‡ç« ID
                            a_tag['data-original-href'] = href
                            a_tag['class'] = 'article-link'  # ç”¨äºå‰ç«¯JavaScriptè¯†åˆ«

                        # æ›¿æ¢åŸæ¥çš„aæ ‡ç­¾
                        link.replace_with(a_tag)
                        logger.debug(f"è½¬æ¢ä¸ºé“¾æ¥: {href} -> {link_type}:{link_data}")
                    elif href.startswith('#') and len(href) > 1:
                        # é¡µé¢å†…é”šç‚¹é“¾æ¥ - ä¿ç•™åŸæ ·ä»¥æ”¯æŒé¡µé¢å†…å¯¼èˆª
                        logger.debug(f"ä¿ç•™é”šç‚¹é“¾æ¥: {href}")
                        # ä¸åšä»»ä½•ä¿®æ”¹ï¼Œä¿æŒåŸæœ‰çš„é”šç‚¹é“¾æ¥
                    else:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯licenseæ–‡ä»¶é“¾æ¥ - åº”è¯¥è½¬æ¢ä¸ºGitHubé¡¹ç›®é“¾æ¥
                        is_license_file = (
                            'license' in href.lower() and 
                            ('.txt' in href.lower() or '.md' in href.lower()) and
                            ('attachments/download' in href or 'files.kf5.com' in href)
                        )
                        
                        if is_license_file:
                            # licenseæ–‡ä»¶è½¬æ¢ä¸ºGitHubé¡¹ç›®é“¾æ¥çš„å ä½ç¬¦
                            logger.debug(f"æ£€æµ‹åˆ°licenseæ–‡ä»¶é“¾æ¥ï¼Œè½¬æ¢ä¸ºGitHubé¡¹ç›®é“¾æ¥: {href}")
                            
                            # å°è¯•ä»é“¾æ¥æ–‡æœ¬ä¸­æå–é¡¹ç›®ä¿¡æ¯
                            github_url = self._extract_github_url_from_license(href, link_text)
                            
                            if github_url:
                                # åˆ›å»ºæŒ‡å‘GitHubé¡¹ç›®çš„å¤–éƒ¨é“¾æ¥
                                link['href'] = github_url
                                link['class'] = 'external-link'
                                link['target'] = '_blank'
                                link['rel'] = 'noopener noreferrer'
                                # æ›´æ–°é“¾æ¥æ–‡æœ¬ï¼Œæ·»åŠ GitHubå›¾æ ‡
                                link.string = f"ğŸ”— {link_text} (GitHubé¡¹ç›®)"
                                logger.debug(f"licenseæ–‡ä»¶è½¬æ¢ä¸ºGitHubé“¾æ¥: {href} -> {github_url}")
                            else:
                                # æ— æ³•ç¡®å®šGitHubé¡¹ç›®ï¼Œè½¬æ¢ä¸ºçº¯æ–‡æœ¬
                                span = soup.new_tag('span')
                                span.string = f"ğŸ“„ {link_text} (é¡¹ç›®è®¸å¯è¯)"
                                span['class'] = 'license-text'
                                span['style'] = 'color: #6b7280; font-weight: normal;'
                                link.replace_with(span)
                                logger.debug(f"licenseæ–‡ä»¶è½¬æ¢ä¸ºçº¯æ–‡æœ¬: {href}")
                        else:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯å…¶ä»–é™„ä»¶ä¸‹è½½é“¾æ¥
                            is_attachment = (
                                'attachments/download' in href or
                                'files.kf5.com/attachments' in href or
                                any(href.lower().endswith(ext) for ext in self.attachment_formats)
                            )
                            
                            if is_attachment:
                                # å°è¯•ä¸‹è½½é™„ä»¶
                                attachment_filename = self.download_attachment(href)
                                
                                if attachment_filename:
                                    # ç”±äºHTMLä½¿ç”¨äº†baseæ ‡ç­¾æŒ‡å‘outputæ ¹ç›®å½•ï¼Œ
                                    # é™„ä»¶è·¯å¾„åº”è¯¥ç›´æ¥åŸºäºoutputç›®å½•ï¼Œä¸éœ€è¦../å‰ç¼€
                                    local_attachment_path = f"attachments/{attachment_filename}"
                                    
                                    # åˆ›å»ºæœ¬åœ°é™„ä»¶é“¾æ¥
                                    a_tag = soup.new_tag('a')
                                    a_tag.string = f"ğŸ“ {link_text}"  # æ·»åŠ é™„ä»¶å›¾æ ‡
                                    a_tag['href'] = local_attachment_path
                                    a_tag['class'] = 'attachment-link'
                                    a_tag['target'] = '_blank'  # åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€
                                    
                                    # å¯¹äºå¯é¢„è§ˆçš„æ–‡ä»¶ï¼ˆå¦‚PDFï¼‰ï¼Œä¸æ·»åŠ downloadå±æ€§ï¼Œè®©æµè§ˆå™¨ç›´æ¥é¢„è§ˆ
                                    # å¯¹äºå…¶ä»–æ–‡ä»¶ï¼Œæ·»åŠ downloadå±æ€§å¼ºåˆ¶ä¸‹è½½
                                    previewable_formats = {'.pdf', '.txt', '.json', '.xml', '.csv'}
                                    file_ext = Path(attachment_filename).suffix.lower()
                                    if file_ext not in previewable_formats:
                                        a_tag['download'] = attachment_filename
                                    
                                    # æ›¿æ¢åŸæ¥çš„aæ ‡ç­¾
                                    link.replace_with(a_tag)
                                    logger.debug(f"è½¬æ¢ä¸ºæœ¬åœ°é™„ä»¶é“¾æ¥: {href} -> {local_attachment_path}")
                                else:
                                    # é™„ä»¶ä¸‹è½½å¤±è´¥ï¼Œæ˜¾ç¤ºä¸ºå¤±è´¥æç¤º
                                    span = soup.new_tag('span')
                                    span.string = f"âŒ {link_text} (ä¸‹è½½å¤±è´¥)"
                                    span['class'] = 'failed-attachment'
                                    span['style'] = 'color: #c92a2a; font-weight: bold; border-bottom: 1px dotted #c92a2a;'
                                    
                                    # æ›¿æ¢åŸæ¥çš„aæ ‡ç­¾
                                    link.replace_with(span)
                                    logger.warning(f"é™„ä»¶ä¸‹è½½å¤±è´¥ï¼Œè½¬æ¢ä¸ºå¤±è´¥æç¤º: {href}")
                            else:
                                # æ£€æŸ¥æ˜¯å¦æ˜¯sectionæˆ–categoryé“¾æ¥
                                if '/hc/kb/section/' in href or '/hc/kb/category/' in href:
                                    # sectionå’Œcategoryé“¾æ¥è½¬æ¢ä¸ºçº¯æ–‡å­—
                                    link_type = 'section' if '/hc/kb/section/' in href else 'category'
                                    logger.debug(f"å°†{link_type}é“¾æ¥è½¬æ¢ä¸ºçº¯æ–‡æœ¬: {href}")
                                    
                                    span = soup.new_tag('span')
                                    span.string = link_text
                                    span['class'] = f'{link_type}-text'
                                    span['style'] = 'color: #6b7280; font-weight: normal;'
                                    
                                    # æ›¿æ¢åŸæ¥çš„aæ ‡ç­¾
                                    link.replace_with(span)
                                    logger.debug(f"sectioné“¾æ¥è½¬æ¢ä¸ºæ–‡å­—: {href}")
                                else:
                                    # å¯¹äºå…¶ä»–å¤–éƒ¨é“¾æ¥ï¼Œä¿æŒä¸ºå¯ç‚¹å‡»çš„è¶…é“¾æ¥
                                    # æ·»åŠ å¤–éƒ¨é“¾æ¥æ ‡è¯†å’Œæ ·å¼
                                    link['class'] = 'external-link'
                                    link['target'] = '_blank'  # åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€
                                    link['rel'] = 'noopener noreferrer'  # å®‰å…¨å±æ€§
                                
                                # ä¿æŒåŸå§‹é“¾æ¥æ–‡æœ¬ï¼Œä¸æ·»åŠ å›¾æ ‡
                                
                                logger.debug(f"ä¿æŒå¤–éƒ¨é“¾æ¥: {href}")
        
        # ä¸ºæ ‡é¢˜æ·»åŠ idå±æ€§ä»¥æ”¯æŒé”šç‚¹å¯¼èˆª
        self._add_heading_ids(soup)
        
        return str(soup), downloaded_files
    
    def _add_heading_ids(self, soup):
        """ä¸ºæ ‡é¢˜æ ‡ç­¾æ·»åŠ idå±æ€§ä»¥æ”¯æŒé”šç‚¹å¯¼èˆª"""
        import re
        
        # å®šä¹‰æ ‡é¢˜ä¸æ­¥éª¤IDçš„æ˜ å°„
        step_mapping = {
            'å‰è¨€': 'step1',
            'è§†é¢‘å­¦ä¹ ': 'step8', 
            'åŠŸèƒ½æ¢³ç†': 'step2',
            'Demoæ¼”ç¤º': 'step3',
            'æ•ˆæœå›¾': 'step3',  # æ•ˆæœå›¾å¯èƒ½æ˜¯Demoæ¼”ç¤ºçš„åˆ«å
            'å¦‚ä½•å®ç°ï¼Ÿ': 'step4',
            'å¦‚ä½•å®ç°': 'step4',
            'ä»£ç åˆ†äº«': 'step5',
            'ä»£ç å…±äº«': 'step5',
            'Demoä»£ç ä½¿ç”¨æ¡æ¬¾': 'step6',
            'æ³¨æ„äº‹é¡¹': 'step7',
            'æœ€å': 'step9'
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰h1-h6æ ‡é¢˜
        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            heading_text = heading.get_text().strip()
            
            # æ¸…ç†æ ‡é¢˜æ–‡æœ¬ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
            clean_text = re.sub(r'[ï¼Ÿ?ï¼!ã€‚.]', '', heading_text)
            
            # æŸ¥æ‰¾åŒ¹é…çš„æ­¥éª¤ID
            step_id = None
            for key, value in step_mapping.items():
                if key in clean_text:
                    step_id = value
                    break
            
            # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„æ­¥éª¤IDï¼Œæ·»åŠ idå±æ€§
            if step_id and not heading.get('id'):
                heading['id'] = step_id
                logger.debug(f"ä¸ºæ ‡é¢˜ '{heading_text}' æ·»åŠ id: {step_id}")
    
    def get_download_stats(self) -> Dict[str, int]:
        """è·å–ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'images_downloaded': len(self.downloaded_images),
            'attachments_downloaded': len(self.downloaded_attachments),
            'total_downloaded': len(self.downloaded_images) + len(self.downloaded_attachments),
            'failed': len(self.failed_downloads),
            'total_attempted': len(self.downloaded_images) + len(self.downloaded_attachments) + len(self.failed_downloads)
        }
    
    def cleanup_unused_images(self, used_images: Set[str]) -> int:
        """æ¸…ç†æœªä½¿ç”¨çš„å›¾ç‰‡æ–‡ä»¶"""
        if not self.images_dir.exists():
            return 0
        
        cleaned_count = 0
        
        for image_file in self.images_dir.glob('*'):
            if image_file.is_file() and image_file.name not in used_images:
                try:
                    image_file.unlink()
                    cleaned_count += 1
                    logger.debug(f"æ¸…ç†æœªä½¿ç”¨çš„å›¾ç‰‡: {image_file.name}")
                except Exception as e:
                    logger.error(f"æ¸…ç†å›¾ç‰‡å¤±è´¥ {image_file.name}: {e}")
        
        return cleaned_count


class HTMLGenerator:
    """HTMLç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.html_dir = output_dir / "html"
        self.html_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_article_html(self, article, html_content: str, images: List[str] = None) -> Path:
        """ç”Ÿæˆå•ä¸ªæ–‡ç« çš„HTMLæ–‡ä»¶"""
        if not article.title:
            return None
        
        # åˆ›å»ºåˆ†ç±»ç›®å½•
        category_parts = article.category.split('/') if hasattr(article, 'category') and article.category else ['å…¶ä»–']
        category_dir = self.html_dir
        
        for part in category_parts:
            category_dir = category_dir / get_safe_filename(part)
        category_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”ŸæˆHTMLæ–‡ä»¶å (åŒ…å«æ–‡ç« IDä»¥ä¾¿é“¾æ¥)
        import re
        article_id = ""
        if hasattr(article, 'url') and article.url:
            id_match = re.search(r'/hc/kb/article/(\d+)', article.url)
            if id_match:
                article_id = id_match.group(1)

        safe_title = get_safe_filename(article.title)
        if article_id:
            html_file = category_dir / f"{article_id}_{safe_title}.html"
        else:
            html_file = category_dir / f"{safe_title}.html"
        
        # ç”ŸæˆHTMLå†…å®¹
        html_template = self._get_html_template()

        # è®¡ç®—åˆ†ç±»å±‚çº§æ·±åº¦
        category_depth = len(category_parts)
        
        # è®¡ç®—baseè·¯å¾„ - å›åˆ°outputç›®å½•çš„æ ¹ç›®å½•
        # éœ€è¦é¢å¤–çš„ä¸€å±‚"../"æ¥å›åˆ°htmlç›®å½•çš„ä¸Šä¸€çº§ï¼ˆoutput_tinyæ ¹ç›®å½•ï¼‰
        base_path = "../" * (category_depth + 1)
        
        # ç”±äºHTMLä½¿ç”¨äº†baseæ ‡ç­¾æŒ‡å‘outputæ ¹ç›®å½•ï¼Œ
        # æ‰€æœ‰è·¯å¾„éƒ½åº”è¯¥ç›´æ¥åŸºäºoutputç›®å½•ï¼Œä¸éœ€è¦../å‰ç¼€
        index_link = "index.html"
        css_path = "css/article.css"
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            'title': article.title,
            'category': getattr(article, 'category', ''),
            'section': getattr(article, 'section_title', ''),
            'last_updated': getattr(article, 'last_updated', ''),
            'scraped_at': getattr(article, 'scraped_at', ''),
            'content_length': f"{getattr(article, 'content_length', 0)} å­—ç¬¦",
            'images_count': len(getattr(article, 'image_paths', []))
        }

        # å¡«å……æ¨¡æ¿
        final_html = html_template.replace('{title}', str(metadata['title']))
        final_html = final_html.replace('{category}', str(metadata['category']))
        final_html = final_html.replace('{section}', str(metadata['section']))
        final_html = final_html.replace('{last_updated}', str(metadata['last_updated']))
        final_html = final_html.replace('{scraped_at}', str(metadata['scraped_at']))
        final_html = final_html.replace('{content_length}', str(metadata['content_length']))
        final_html = final_html.replace('{images_count}', str(metadata['images_count']))
        final_html = final_html.replace('{content}', str(html_content))
        final_html = final_html.replace('{index_link}', index_link)
        final_html = final_html.replace('{css_path}', css_path)
        final_html = final_html.replace('{base_path}', base_path)
        
        # ä¿å­˜HTMLæ–‡ä»¶
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(final_html)
        
        logger.debug(f"HTMLæ–‡ä»¶å·²ç”Ÿæˆ: {html_file}")
        return html_file
    
    def _get_html_template(self) -> str:
        """è·å–HTMLæ¨¡æ¿"""
        return '''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title} - kintoneå¼€å‘è€…æ–‡æ¡£</title>
    <base href="{base_path}">
    <link rel="stylesheet" href="{css_path}">
    <!-- Prism syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs/plugins/line-numbers/prism-line-numbers.min.css">
</head>
<body>
    <nav class="navbar">
        <div class="navbar-content">
            <a href="{index_link}" class="navbar-brand">ğŸ“š kintoneå¼€å‘è€…æ–‡æ¡£</a>
            <div class="navbar-links">
                <a href="{index_link}" class="navbar-link back-to-home">â† è¿”å›é¦–é¡µ</a>
            </div>
        </div>
    </nav>

    <div class="header">
        <h1>{title}</h1>
        <div class="metadata">
            <div class="metadata-item">
                <span class="metadata-label">ğŸ“‚ åˆ†ç±»:</span>
                <span class="metadata-value">{category}</span>
            </div>
            <div class="metadata-item">
                <span class="metadata-label">ğŸ“Š é•¿åº¦:</span>
                <span class="metadata-value">{content_length}</span>
            </div>
        </div>
    </div>
    
    <div class="content">
        {content}
    </div>
    
    <div class="footer">
        <p>æœ¬æ–‡æ¡£ç”± kintone-scraper è‡ªåŠ¨æŠ“å–ç”Ÿæˆ</p>
        <p>åŸå§‹å†…å®¹ç‰ˆæƒå½’ cybozu æ‰€æœ‰</p>
    </div>
    
    <a href="#" class="back-to-top" onclick="window.scrollTo(0,0); return false;">â†‘</a>

    <!-- Prism core + autoloader -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
      (function(){
        if (window.Prism && Prism.plugins && Prism.plugins.autoloader) {
          Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs/components/';
        }
        function inferLanguage(text){
          const t = (text || '').trim();
          if (!t) return null;
          if (/^\{[\s\S]*\}$/.test(t) || /^\[/.test(t)) { try { JSON.parse(t); return 'json'; } catch(e){} }
          if (/<\/?[a-zA-Z]/.test(t)) return 'markup';
          if (/^(\$ |curl |#\!\/|sudo |apt |yum |brew )/m.test(t)) return 'bash';
          if (/(import |from |def |class |print\(|lambda )/.test(t)) return 'python';
          if (/(const |let |var |=>|function\s+\w+\()/.test(t)) return 'javascript';
          if (/(SELECT |INSERT |UPDATE |DELETE |CREATE TABLE)/i.test(t)) return 'sql';
          return null;
        }
        function mapBrushToPrism(brush){
          const m = String(brush || '').toLowerCase();
          const map = { js:'javascript', javascript:'javascript', ts:'typescript', typescript:'typescript',
                        html:'markup', xml:'markup', markup:'markup', json:'json', css:'css',
                        bash:'bash', shell:'bash', sh:'bash', sql:'sql', java:'java', py:'python', python:'python',
                        yaml:'yaml', yml:'yaml', ini:'ini', txt:'none' };
          return map[m] || null;
        }
        function enhanceCodeBlocks(root){
          const container = root || document;
          const pres = Array.from(container.querySelectorAll('pre'));
          pres.forEach(pre => {
            let lang = null;
            const cls = pre.getAttribute('class') || '';
            const m = cls.match(/brush:([\w-]+)/i);
            if (m) lang = mapBrushToPrism(m[1]);
            let code = pre.querySelector('code');
            if (code) {
              const codeCls = code.getAttribute('class') || '';
              const mm = codeCls.match(/language-([\w-]+)/i);
              if (mm) lang = mm[1];
            }
            if (!lang) {
              const text = (code ? code.textContent : pre.textContent) || '';
              lang = inferLanguage(text) || 'none';
            }
            // ensure we have a <code> child that contains only code text (not action buttons)
            const actions = pre.querySelector('.code-actions');
            if (!code) {
              const rawText = pre.textContent || '';
              // clear pre and reconstruct
              pre.innerHTML = '';
              code = document.createElement('code');
              code.textContent = rawText;
              pre.appendChild(code);
              if (actions) pre.appendChild(actions);
            }
            const langClass = 'language-' + lang;
            if (!code.classList.contains(langClass)) code.classList.add(langClass);
            // add line numbers on pre if multiline
            const textForLines = code.textContent || '';
            if (textForLines.indexOf('\\\\n') !== -1) pre.classList.add('line-numbers');
          });
          if (window.Prism && Prism.highlightAllUnder) {
            Prism.highlightAllUnder(container);
          }
        }
        window.enhanceCodeBlocks = enhanceCodeBlocks;
        document.addEventListener('DOMContentLoaded', function(){
          try { enhanceCodeBlocks(document); } catch(e) {}
        });
      })();
    </script>
    <script>
      // ç«™å†…é“¾æ¥ï¼ˆarticle-linkï¼‰åœ¨å•é¡µæ–‡ç« å†…çš„å¤„ç†ï¼šè·³è½¬åˆ°é¦–é¡µå¹¶å®šä½åˆ°å¯¹åº”æ–‡ç« ï¼›è‹¥é¦–é¡µæ— è¯¥æ–‡ç« ï¼Œå…œåº•æ‰“å¼€åŸå§‹é“¾æ¥
      document.addEventListener('click', function(e){
        var el = e.target && e.target.closest ? e.target.closest('a.article-link') : null;
        if (!el) return;
        var aid = el.getAttribute('data-article-id');
        var original = el.getAttribute('data-original-href');
        if (!aid) return;
        e.preventDefault();
        try {
          var indexLink = '{index_link}';
          if (!indexLink) { // å…œåº•ä»å¯¼èˆªå–
            var back = document.querySelector('.navbar .back-to-home');
            indexLink = (back && back.getAttribute('href')) || 'index.html';
          }
          if (indexLink.indexOf('#') !== -1) indexLink = indexLink.split('#')[0];
          var target = indexLink + '#' + String(aid);
          // file:// ä¸‹æ— æ³•æ¢æµ‹é¦–é¡µæ˜¯å¦åŒ…å«è¯¥IDï¼Œè¿™é‡Œç›´æ¥è·³é¦–é¡µï¼›
          // é¦–é¡µè‹¥æ‰¾ä¸åˆ°ä¼šæœ‰æç¤ºï¼›å¦‚éœ€å…œåº•åˆ°åŸå§‹é“¾æ¥ï¼Œè¿½åŠ ä¸€æ¬¡è·³è½¬
          window.location.href = target;
          // å»¶è¿Ÿå…œåº•ï¼šè‹¥ç”¨æˆ·è¿”å›æˆ–é¦–é¡µæ— å†…å®¹ï¼Œå¯ç‚¹å‡»å†å²è¿”å›åå†æ¬¡ç‚¹å‡»è§¦å‘ original
          if (original) {
            setTimeout(function(){ try { console.debug('fallback to original link if needed'); } catch(e){} }, 0);
          }
        } catch(err) {
          // æœ€åçš„å…œåº•ï¼šä¿æŒåŸ href è¡Œä¸ºæˆ–æ‰“å¼€åŸå§‹é“¾æ¥
          if (original) {
            window.location.href = original;
          } else {
            window.location.hash = String(aid);
          }
        }
      }, false);
    </script>
</body>
</html>'''
    
    def _copy_css_files(self):
        """å¤åˆ¶CSSæ–‡ä»¶åˆ°è¾“å‡ºç›®å½•"""
        import shutil
        from pathlib import Path
        
        # åˆ›å»ºcssç›®å½•
        css_dir = self.output_dir / "css"
        css_dir.mkdir(exist_ok=True)
        
        # è·å–CSSæºæ–‡ä»¶è·¯å¾„
        css_source_dir = Path(__file__).parent / "css"
        
        # å¤åˆ¶CSSæ–‡ä»¶
        if css_source_dir.exists():
            for css_file in css_source_dir.glob("*.css"):
                dest_file = css_dir / css_file.name
                shutil.copy2(css_file, dest_file)
                print(f"å·²å¤åˆ¶CSSæ–‡ä»¶: {css_file.name} -> {dest_file}")
        else:
            print(f"CSSæºç›®å½•ä¸å­˜åœ¨: {css_source_dir}")

    def generate_index_html(self, categories: List, articles: List) -> Path:
        """ç”ŸæˆVueé£æ ¼çš„ç´¢å¼•é¡µé¢"""
        # index.htmlåº”è¯¥åœ¨è¾“å‡ºç›®å½•çš„æ ¹ç›®å½•ï¼Œå’Œhtmlã€imagesç›®å½•å¹³çº§
        index_file = self.output_dir / "index.html"
        
        # å¤åˆ¶CSSæ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
        self._copy_css_files()
        
        # è‹¥ä¼ å…¥çš„ articles æœªåŒ…å«å…¨éƒ¨ç°æœ‰æ–‡ä»¶ï¼ˆä¾‹å¦‚å¯ç”¨å¢é‡è·³è¿‡æ—¶ï¼‰ï¼Œ
        # ä» html ç›®å½•è¡¥å…¨æœ¬åœ°å·²å­˜åœ¨çš„æ–‡ç« ï¼Œä¿è¯ç´¢å¼•å®Œæ•´
        try:
            augmented = self._augment_articles_from_files(articles)
            articles = augmented
        except Exception as e:
            logger.warning(f"è¡¥å…¨æœ¬åœ°æ–‡ç« åˆ—è¡¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åˆ—è¡¨: {e}")
        
        # ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for article in articles:
            cat = getattr(article, 'category', '') or 'å…¶ä»–'
            category_stats[cat] = category_stats.get(cat, 0) + 1
        
        # ç”Ÿæˆå¯¼èˆªæ ‘å’Œæ–‡ç« å†…å®¹
        navigation_tree_html = self._generate_navigation_tree(articles)
        article_contents_html = self._generate_article_contents(articles)

        template = self._get_index_template()
        html_content = template.replace('{total_articles}', str(len(articles)))
        html_content = html_content.replace('{total_categories}', str(len(category_stats)))
        html_content = html_content.replace('{navigation_tree}', navigation_tree_html)
        html_content = html_content.replace('{article_contents}', article_contents_html)
        
        with open(index_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        # ä¿®å¤æ‰€æœ‰æ–‡ç« é—´çš„é“¾æ¥
        self._fix_article_links(articles)
        
        # ä¿®å¤index.htmlä¸­çš„é“¾æ¥
        self._fix_index_html_links(index_file, articles)
        
        logger.info(f"ç´¢å¼•é¡µé¢å·²ç”Ÿæˆ: {index_file}")
        return index_file

    def _augment_articles_from_files(self, articles: List) -> List:
        """å°†ç£ç›˜ä¸Šå·²æœ‰çš„ html æ–‡ç« è¡¥å……åˆ°åˆ—è¡¨ä¸­ï¼Œé¿å…ç´¢å¼•ç¼ºç¯‡ã€‚
        - ä» html/ é€’å½’æ‰«æ {category_path}/{id}_{title}.html
        - è‹¥ä¼ å…¥åˆ—è¡¨ä¸­æ²¡æœ‰è¯¥ idï¼Œåˆ™åˆ›å»ºä¸€ä¸ªè½»é‡â€œæ–‡ç« å¯¹è±¡â€è¡¥ä¸Š
        """
        from types import SimpleNamespace
        import re, os
        html_root = self.html_dir
        if not html_root.exists():
            return articles
        
        # å·²æœ‰æ–‡ç« çš„IDé›†åˆï¼ˆä»urlé‡Œæå–ï¼‰
        existing_ids = set()
        for a in articles:
            if hasattr(a, 'url') and a.url:
                m = re.search(r'/hc/kb/article/(\d+)', a.url)
                if m:
                    existing_ids.add(m.group(1))
        
        # æ‰«æç£ç›˜æ–‡ä»¶
        augmented = list(articles)
        for file in html_root.rglob('*.html'):
            if file.name.lower() == 'index.html':
                continue
            # ç›¸å¯¹ç±»åˆ«è·¯å¾„
            rel = file.relative_to(html_root)
            parts = list(rel.parts)
            if not parts:
                continue
            filename = parts[-1]
            cat_parts = parts[:-1]
            category = '/'.join(cat_parts) if cat_parts else 'å…¶ä»–'
            m = re.match(r'^(\d+)_([^\\/]+)\.html$', filename)
            if not m:
                continue
            aid, title = m.group(1), m.group(2)
            if aid in existing_ids:
                continue
            # æ„é€ æœ€å°å­—æ®µé›†åˆï¼ˆä¾›å¯¼èˆªä¸å†…å®¹æ¸²æŸ“ï¼‰
            url = f"/hc/kb/article/{aid}/"
            augmented.append(SimpleNamespace(url=url, title=title, category=category))
        return augmented
    
    def _get_index_template(self) -> str:
        """è·å–ç´¢å¼•é¡µé¢æ¨¡æ¿ - Vueæ–‡æ¡£é£æ ¼"""
        return """<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kintoneå¼€å‘è€…æ–‡æ¡£ - ç¦»çº¿ç‰ˆæœ¬</title>
    
    <!-- ç°ä»£åŒ–å›¾æ ‡å­—ä½“ -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />
    <link rel="stylesheet" href="css/index.css">
    <!-- Prism syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs/plugins/line-numbers/prism-line-numbers.min.css">
</head>
<body>
    <div class="header">
        <h1>kintoneå¼€å‘è€…æ–‡æ¡£</h1>
        <div class="stats">
            <span>{total_articles} ç¯‡æ–‡ç« </span>
            <span>{total_categories} ä¸ªåˆ†ç±»</span>
        </div>
    </div>

    <div class="main-container">
        <div class="sidebar">
            <div class="nav-tree">
                {navigation_tree}
            </div>
        </div>
        
        <div class="content-area">
            <div class="content-welcome" id="welcome-content">
                <h2>ğŸ“š kintoneå¼€å‘è€…æ–‡æ¡£</h2>
                <p>ç‚¹å‡»å·¦ä¾§å¯¼èˆªé€‰æ‹©è¦æŸ¥çœ‹çš„æ–‡ç« </p>
            </div>
            
            <div class="loading" id="loading">
                <p>åŠ è½½ä¸­...</p>
            </div>
            
            {article_contents}
        </div>
    </div>

    <!-- Prism core + autoloader -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/autoloader/prism-autoloader.min.js"></script>

    <script>
        let currentArticle = '';
        let articlesData = {};
        const HASH_PREFIX = 'article-';

        function makeArticleHash(articleId, sectionId) {
            if (!articleId) {
                return '';
            }
            const encodedId = encodeURIComponent(String(articleId));
            let hash = '#' + HASH_PREFIX + encodedId;
            if (sectionId) {
                hash += ':' + encodeURIComponent(String(sectionId));
            }
            return hash;
        }

        function parseArticleHash(hash) {
            if (!hash) {
                return null;
            }
            const raw = hash.replace(/^#/, '');
            if (!raw.startsWith(HASH_PREFIX)) {
                return null;
            }
            const remainder = raw.slice(HASH_PREFIX.length);
            const splitIndex = remainder.indexOf(':');
            const idPart = splitIndex === -1 ? remainder : remainder.slice(0, splitIndex);
            const sectionPart = splitIndex === -1 ? '' : remainder.slice(splitIndex + 1);
            if (!idPart) {
                return null;
            }
            return {
                articleId: decodeURIComponent(idPart),
                sectionId: sectionPart ? decodeURIComponent(sectionPart) : null
            };
        }

        function escapeForSelector(value) {
            if (window.CSS && window.CSS.escape) {
                return window.CSS.escape(value);
            }
            return String(value).replace(/[^a-zA-Z0-9_-]/g, '\\\\$&');
        }

        function focusSection(articleId, sectionId, smooth) {
            if (!articleId || !sectionId) {
                return;
            }
            const articleContent = document.getElementById('article-' + articleId);
            if (!articleContent) {
                return;
            }
            const safeSection = escapeForSelector(sectionId);
            let target = articleContent.querySelector('#' + safeSection);
            if (!target) {
                target = articleContent.querySelector('[id="' + safeSection + '"]');
            }
            if (!target) {
                target = articleContent.querySelector('[name="' + safeSection + '"]');
            }
            if (target && target.scrollIntoView) {
                target.scrollIntoView({ behavior: smooth ? 'smooth' : 'auto', block: 'start' });
            }
        }

        function updateLocationHash(articleId, sectionId) {
            const targetHash = makeArticleHash(articleId, sectionId);
            if (!targetHash) {
                return;
            }
            if (window.location.hash !== targetHash) {
                window.location.hash = targetHash;
            }
        }


        // ä»£ç é«˜äº®å¢å¼ºï¼ˆPrism + brushæ˜ å°„ + å¯å‘å¼ï¼‰
        (function(){
            if (window.Prism && Prism.plugins && Prism.plugins.autoloader) {
                Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs/components/';
            }
            function inferLanguage(text){
              const t = (text || '').trim();
              if (!t) return null;
              if (/^\{[\s\S]*\}$/.test(t) || /^\[/.test(t)) { try { JSON.parse(t); return 'json'; } catch(e){} }
              if (/<\/?[a-zA-Z]/.test(t)) return 'markup';
              if (/^(\$ |curl |#\!\/|sudo |apt |yum |brew )/m.test(t)) return 'bash';
              if (/(import |from |def |class |print\(|lambda )/.test(t)) return 'python';
              if (/(const |let |var |=>|function\s+\w+\()/.test(t)) return 'javascript';
              if (/(SELECT |INSERT |UPDATE |DELETE |CREATE TABLE)/i.test(t)) return 'sql';
              return null;
            }
            function mapBrushToPrism(brush){
              const m = String(brush || '').toLowerCase();
              const map = { js:'javascript', javascript:'javascript', ts:'typescript', typescript:'typescript',
                            html:'markup', xml:'markup', markup:'markup', json:'json', css:'css',
                            bash:'bash', shell:'bash', sh:'bash', sql:'sql', java:'java', py:'python', python:'python',
                            yaml:'yaml', yml:'yaml', ini:'ini', txt:'none' };
              return map[m] || null;
            }
            window.enhanceCodeBlocks = function(root){
              const container = root || document;
              const pres = Array.from(container.querySelectorAll('pre'));
              pres.forEach(pre => {
                let lang = null;
                const cls = pre.getAttribute('class') || '';
                const m = cls.match(/brush:([\w-]+)/i);
                if (m) lang = mapBrushToPrism(m[1]);
                let code = pre.querySelector('code');
                if (code) {
                  const codeCls = code.getAttribute('class') || '';
                  const mm = codeCls.match(/language-([\w-]+)/i);
                  if (mm) lang = mm[1];
                }
                if (!lang) {
                  const text = (code ? code.textContent : pre.textContent) || '';
                  lang = inferLanguage(text) || 'none';
                }
                const actions = pre.querySelector('.code-actions');
                if (!code) {
                  const rawText = pre.textContent || '';
                  pre.innerHTML = '';
                  code = document.createElement('code');
                  code.textContent = rawText;
                  pre.appendChild(code);
                  if (actions) pre.appendChild(actions);
                }
                const langClass = 'language-' + lang;
                if (!code.classList.contains(langClass)) code.classList.add(langClass);
                const textForLines = code.textContent || '';
                if (textForLines.indexOf('\\\\n') !== -1) pre.classList.add('line-numbers');
              });
              if (window.Prism && Prism.highlightAllUnder) {
                Prism.highlightAllUnder(container);
              }
            };
        })();

        // ç°ä»£åŒ–æ ‘å½¢å¯¼èˆªæ§åˆ¶
        function toggleTreeNode(nodeId) {
            const node = document.getElementById(nodeId);
            if (!node) {
                console.error('Node not found:', nodeId);
                return;
            }
            
            const header = node.querySelector('.tree-node-header');
            const children = node.querySelector('.tree-node-children');
            const expandIcon = header ? header.querySelector('.tree-icon.expandable') : null;
            
            if (node.classList.contains('expanded')) {
                // æ”¶èµ·
                node.classList.remove('expanded');
                if (children) {
                    // ç§»é™¤å†…è”æ ·å¼ï¼Œè®©CSSç±»æ§åˆ¶
                    children.style.removeProperty('max-height');
                    children.style.removeProperty('opacity');
                }
                if (expandIcon) expandIcon.style.transform = 'rotate(0deg)';
            } else {
                // å±•å¼€
                node.classList.add('expanded');
                if (children) {
                    // ç§»é™¤å†…è”æ ·å¼ï¼Œè®©CSSç±»æ§åˆ¶
                    children.style.removeProperty('max-height');
                    children.style.removeProperty('opacity');
                }
                if (expandIcon) expandIcon.style.transform = 'rotate(90deg)';
            }
        }
        

        // æ˜¾ç¤ºæ–‡ç« å†…å®¹
        function showArticle(articleId, options = {}) {
            const { sectionId = null, updateHash = true, scrollIntoView = true } = options || {};

            // éšè—æ¬¢è¿é¡µé¢
            document.getElementById('welcome-content').style.display = 'none';

            // éšè—æ‰€æœ‰æ–‡ç« å†…å®¹
            document.querySelectorAll('.article-content').forEach(function(content) {
                content.classList.remove('active');
            });

            // æ˜¾ç¤ºé€‰ä¸­çš„æ–‡ç« 
            const articleContent = document.getElementById('article-' + articleId);
            if (articleContent) {
                articleContent.classList.add('active');
                currentArticle = articleId;

                // ä¸ºå½“å‰æ–‡ç« çš„ä»£ç å—æ·»åŠ å¤åˆ¶æŒ‰é’®
                initCodeCopyButtons(articleContent);
                try { enhanceCodeBlocks(articleContent); } catch (e) {}

                setupInternalAnchors(articleContent, articleId);

                if (!updateHash && sectionId && scrollIntoView) {
                    focusSection(articleId, sectionId, scrollIntoView);
                }

                if (updateHash) {
                    updateLocationHash(articleId, sectionId);
                }
            } else {
                // å¦‚æœæ–‡ç« ä¸å­˜åœ¨ï¼Œæ˜¾ç¤ºå‹å¥½æç¤º
                const missingMessage = 'æ–‡ç«  ID ' + articleId + ' æœªåŒ…å«åœ¨å½“å‰ç¦»çº¿æ–‡æ¡£ä¸­ã€‚' + '\\n\\n' + 'è¿™å¯èƒ½æ˜¯å› ä¸ºè¯¥æ–‡ç« åœ¨å…¶ä»–åˆ†ç±»ä¸­ï¼Œæˆ–è€…éœ€è¦å®Œæ•´æŠ“å–æ‰èƒ½è·å–ã€‚';
                alert(missingMessage);
            }
        }

        function setupInternalAnchors(articleContent, articleId) {
            if (!articleContent) {
                return;
            }
            const anchors = articleContent.querySelectorAll('a[href^="#"]');
            anchors.forEach(function(anchor) {
                if (anchor.dataset && anchor.dataset.hashBound === '1') {
                    return;
                }
                if (anchor.dataset) {
                    anchor.dataset.hashBound = '1';
                } else {
                    anchor.setAttribute('data-hash-bound', '1');
                }
                anchor.addEventListener('click', function(event) {
                    const href = anchor.getAttribute('href');
                    if (!href || href === '#') {
                        return;
                    }
                    const rawSection = href.slice(1);
                    if (!rawSection) {
                        return;
                    }
                    event.preventDefault();
                    let decodedSection = rawSection;
                    try { decodedSection = decodeURIComponent(rawSection); } catch (err) {}
                    const targetHash = makeArticleHash(articleId, decodedSection);
                    if (window.location.hash === targetHash) {
                        focusSection(articleId, decodedSection, true);
                    } else {
                        window.location.hash = targetHash;
                    }
                });
            });
        }

        // åˆå§‹åŒ–ä»£ç å¤åˆ¶æŒ‰é’®
        function initCodeCopyButtons(container) {
            const preBlocks = container.querySelectorAll('pre');
            preBlocks.forEach(function(pre) {
                // æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æŒ‰é’®å®¹å™¨
                if (pre.querySelector('.code-actions')) {
                    return;
                }
                
                // æ·»åŠ æ ‡è®°ç±»
                pre.classList.add('has-actions');
                
                // åˆ›å»ºæŒ‰é’®å®¹å™¨
                const actionsContainer = document.createElement('div');
                actionsContainer.className = 'code-actions';
                
                // åˆ›å»ºæ¢è¡Œåˆ‡æ¢æŒ‰é’®
                const wrapBtn = document.createElement('button');
                wrapBtn.className = 'wrap-btn';
                wrapBtn.textContent = 'æ¢è¡Œ';
                wrapBtn.title = 'åˆ‡æ¢ä»£ç æ¢è¡Œ';
                wrapBtn.onclick = function() {
                    pre.classList.toggle('wrapped');
                    if (pre.classList.contains('wrapped')) {
                        wrapBtn.textContent = 'ä¸æ¢è¡Œ';
                        wrapBtn.classList.add('active');
                    } else {
                        wrapBtn.textContent = 'æ¢è¡Œ';
                        wrapBtn.classList.remove('active');
                    }
                };
                
                // åˆ›å»ºå¤åˆ¶æŒ‰é’®
                const copyBtn = document.createElement('button');
                copyBtn.className = 'copy-btn';
                copyBtn.textContent = 'å¤åˆ¶';
                copyBtn.title = 'å¤åˆ¶ä»£ç ';
                copyBtn.onclick = function() {
                    const codeText = pre.textContent || pre.innerText;
                    // ç§»é™¤æŒ‰é’®æ–‡æœ¬
                    const textToCopy = codeText.replace(/^(æ¢è¡Œ|ä¸æ¢è¡Œ)?\s*å¤åˆ¶\s*/, '');
                    
                    copyToClipboard(textToCopy, function(success) {
                        if (success) {
                            copyBtn.textContent = 'å·²å¤åˆ¶!';
                            copyBtn.classList.add('copied');
                            setTimeout(function() {
                                copyBtn.textContent = 'å¤åˆ¶';
                                copyBtn.classList.remove('copied');
                            }, 2000);
                        } else {
                            copyBtn.textContent = 'å¤±è´¥';
                            setTimeout(function() {
                                copyBtn.textContent = 'å¤åˆ¶';
                            }, 2000);
                        }
                    });
                };
                
                // æ·»åŠ æŒ‰é’®åˆ°å®¹å™¨
                actionsContainer.appendChild(wrapBtn);
                actionsContainer.appendChild(copyBtn);
                pre.appendChild(actionsContainer);
            });
        }
        
        // å¤åˆ¶åˆ°å‰ªè´´æ¿çš„è¾…åŠ©å‡½æ•°
        function copyToClipboard(text, callback) {
            if (navigator.clipboard && navigator.clipboard.writeText) {
                navigator.clipboard.writeText(text).then(
                    function() { callback(true); },
                    function() { callback(false); }
                );
            } else {
                // æ—§ç‰ˆæµè§ˆå™¨çš„å…¼å®¹æ–¹æ¡ˆ
                const textArea = document.createElement('textarea');
                textArea.value = text;
                textArea.style.position = 'fixed';
                textArea.style.left = '-999999px';
                document.body.appendChild(textArea);
                textArea.focus();
                textArea.select();
                
                try {
                    const successful = document.execCommand('copy');
                    callback(successful);
                } catch (err) {
                    callback(false);
                }
                
                document.body.removeChild(textArea);
            }
        }

        function handleHashNavigation(scrollToTarget) {
            if (scrollToTarget === undefined) {
                scrollToTarget = true;
            }
            const info = parseArticleHash(window.location.hash);
            if (info) {
                showArticle(info.articleId, { updateHash: false, sectionId: info.sectionId, scrollIntoView: scrollToTarget });
                if (!info.sectionId && scrollToTarget) {
                    if (typeof window.scrollTo === 'function') {
                        window.scrollTo({ top: 0, behavior: 'auto' });
                    }
                }
            } else if (!window.location.hash) {
                showWelcome();
            }
        }

        // æ˜¾ç¤ºæ¬¢è¿é¡µé¢
        function showWelcome() {
            document.getElementById('welcome-content').style.display = 'flex';
            
            // éšè—æ‰€æœ‰æ–‡ç« å†…å®¹
            document.querySelectorAll('.article-content').forEach(content => {
                content.classList.remove('active');
            });
            
            currentArticle = '';
        }

        // åˆå§‹åŒ–é¡µé¢
        document.addEventListener('DOMContentLoaded', function() {
            // ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹é»˜è®¤æ˜¯æŠ˜å çŠ¶æ€ï¼ˆé€šè¿‡ç§»é™¤expandedç±»ï¼‰
            document.querySelectorAll('.tree-node').forEach(function(node) {
                node.classList.remove('expanded');
            });
            handleHashNavigation(false);
        });

        window.addEventListener('hashchange', function() {
            handleHashNavigation(true);
        });
    </script>
</body>
</html>"""

    def _generate_navigation_tree(self, articles: List) -> str:
        """ç”ŸæˆVueé£æ ¼çš„å¯¼èˆªæ ‘HTML"""
        # æŒ‰åˆ†ç±»ç»„ç»‡æ–‡ç« 
        categories = {}
        for article in articles:
            category = getattr(article, 'category', '') or 'å…¶ä»–'
            if category not in categories:
                categories[category] = []
            categories[category].append(article)
        
        # ç»„ç»‡æˆå±‚çº§ç»“æ„
        hierarchy = {}
        for category, articles_list in categories.items():
            parts = (category or 'å…¶ä»–').split('/')
            if len(parts) >= 2:
                parent = parts[0]
                child = parts[1]
                if parent not in hierarchy:
                    hierarchy[parent] = {}
                hierarchy[parent][child] = articles_list
            else:
                # å•çº§åˆ†ç±»
                if category not in hierarchy:
                    hierarchy[category] = {}
                hierarchy[category]['_articles'] = articles_list
        
        # å®šä¹‰åˆ†ç±»æ˜¾ç¤ºé¡ºåº
        category_order = [
            "æ–°æ‰‹æ•™ç¨‹", "APIæ–‡æ¡£", "å·¥å…·", "æ’ä»¶", "å¼€å‘èŒƒä¾‹", "åº”ç”¨åœºæ™¯",
            "å…¶ä»–", "å¼€å‘å­¦ä¹ è§†é¢‘ä¸“æ ", "é€šçŸ¥", "è´¦å·&åè®®"
        ]
        
        # æŒ‰ç…§æŒ‡å®šé¡ºåºæ’åºåˆ†ç±»
        def sort_categories(item):
            category = item[0]
            try:
                return category_order.index(category)
            except ValueError:
                # å¦‚æœåˆ†ç±»ä¸åœ¨æŒ‡å®šåˆ—è¡¨ä¸­ï¼Œæ”¾åˆ°æœ€å
                return len(category_order)
        
        sorted_hierarchy = sorted(hierarchy.items(), key=sort_categories)
        
        # ç”Ÿæˆç°ä»£åŒ–HTMLç»“æ„
        html_parts = []
        for parent_category, children in sorted_hierarchy:
            # ä¸»åˆ†ç±»èŠ‚ç‚¹
            safe_parent = parent_category.replace('/', '-').replace(' ', '-')
            html_parts.append(f'''
            <div class="tree-node level-1" id="node-{safe_parent}">
                <div class="tree-node-header" onclick="toggleTreeNode('node-{safe_parent}')">
                    <i class="tree-icon expandable fas fa-chevron-right"></i>
                    <i class="tree-icon fas fa-folder"></i>
                    <span class="tree-text">{parent_category}</span>
                </div>
                <div class="tree-node-children">''')
            
            # å­åˆ†ç±»æˆ–ç›´æ¥æ–‡ç« 
            for child_name, articles_list in children.items():
                if child_name == '_articles':
                    # ç›´æ¥æ˜¾ç¤ºæ–‡ç« ï¼ˆæ²¡æœ‰å­åˆ†ç±»ï¼‰
                    for article in articles_list:
                        article_id = self._extract_article_id(article)
                        html_parts.append(f'''
                    <div class="tree-node level-3">
                        <div class="tree-node-header" onclick="showArticle('{article_id}')">
                            <i class="tree-icon fas fa-file-alt"></i>
                            <span class="tree-text">{article.title}</span>
                        </div>
                    </div>''')
                else:
                    # å­åˆ†ç±»èŠ‚ç‚¹
                    safe_child = f"{safe_parent}-{child_name.replace('/', '-').replace(' ', '-')}"
                    html_parts.append(f'''
                    <div class="tree-node level-2" id="node-{safe_child}">
                        <div class="tree-node-header" onclick="toggleTreeNode('node-{safe_child}')">
                            <i class="tree-icon expandable fas fa-chevron-right"></i>
                            <i class="tree-icon fas fa-folder-open"></i>
                            <span class="tree-text">{child_name}</span>
                            <span class="article-count">{len(articles_list)}</span>
                        </div>
                        <div class="tree-node-children">''')
                    
                    # å­åˆ†ç±»ä¸‹çš„æ–‡ç« 
                    for article in articles_list:
                        article_id = self._extract_article_id(article)
                        html_parts.append(f'''
                            <div class="tree-node level-3">
                                <div class="tree-node-header" onclick="showArticle('{article_id}')">
                                    <i class="tree-icon fas fa-file-alt"></i>
                                    <span class="tree-text">{article.title}</span>
                                </div>
                            </div>''')
                    
                    html_parts.append('                        </div>\n                    </div>')
            
            html_parts.append('                </div>\n            </div>')
        
        return '\n'.join(html_parts)
    
    def _fix_image_paths_for_index(self, html_content: str) -> str:
        """ä¿®å¤HTMLå†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼Œé€‚åº”ä¸»é¡µé¢index.htmlçš„ä½ç½®"""
        import re
        
        # å°† ../../../images/ æ›¿æ¢ä¸º images/
        # è¿™æ˜¯å› ä¸ºæ–‡ç« é¡µé¢åœ¨ html/åˆ†ç±»/å­åˆ†ç±»/ ä¸­ï¼Œè€Œä¸»é¡µé¢åœ¨æ ¹ç›®å½•ä¸­
        html_content = re.sub(r'src="(\.\./)+images/', 'src="images/', html_content)
        
        return html_content
    
    def _extract_article_id(self, article) -> str:
        """ä»æ–‡ç« URLä¸­æå–ID"""
        import re
        if hasattr(article, 'url') and article.url:
            id_match = re.search(r'/hc/kb/article/(\d+)', article.url)
            if id_match:
                return id_match.group(1)
        # å¦‚æœæ²¡æœ‰IDï¼Œä½¿ç”¨å®‰å…¨çš„æ ‡é¢˜ä½œä¸ºID
        return get_safe_filename(article.title)[:20]
    
    def _generate_article_contents(self, articles: List) -> str:
        """ç”Ÿæˆæ‰€æœ‰æ–‡ç« çš„å†…å®¹HTML"""
        contents = []
        
        for article in articles:
            if not hasattr(article, 'title') or not article.title:
                continue
                
            article_id = self._extract_article_id(article)
            
            # è¯»å–æ–‡ç« çš„HTMLå†…å®¹
            category_parts = (getattr(article, 'category', '') or 'å…¶ä»–').split('/')
            category_dir = self.html_dir
            for part in category_parts:
                category_dir = category_dir / get_safe_filename(part)
            
            safe_title = get_safe_filename(article.title)
            import re
            url_id = ""
            if hasattr(article, 'url') and article.url:
                id_match = re.search(r'/hc/kb/article/(\d+)', article.url)
                if id_match:
                    url_id = id_match.group(1)
            
            if url_id:
                html_file = category_dir / f"{url_id}_{safe_title}.html"
            else:
                html_file = category_dir / f"{safe_title}.html"
            
            article_content = ""
            if html_file.exists():
                try:
                    with open(html_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # æå–bodyå†…å®¹
                        from bs4 import BeautifulSoup
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # å°è¯•å¤šç§å¯èƒ½çš„å†…å®¹å®¹å™¨ï¼Œä½†æå–å…¶å†…éƒ¨HTMLè€Œéæ•´ä¸ªå®¹å™¨
                        body_content = (soup.find('div', class_='article-body') or 
                                      soup.find('div', class_='content-body') or 
                                      soup.find('div', class_='main-content') or
                                      soup.find('main') or
                                      soup.find('article'))
                        
                        if body_content:
                            # æ¸…ç†å’Œæå–å†…å®¹ï¼Œé¿å…é‡å¤æ ‡é¢˜å’ŒåµŒå¥—ç»“æ„
                            content_copy = BeautifulSoup(str(body_content), 'html.parser')
                            
                            # ç§»é™¤é‡å¤çš„æ ‡é¢˜ï¼ˆä¿ç•™ä¸»è¦å†…å®¹ï¼‰
                            for header in content_copy.find_all(['header', '.article-header']):
                                if header:
                                    header.decompose()
                            
                            # ç§»é™¤é‡å¤çš„article-contentåµŒå¥—
                            nested_content = content_copy.find('div', class_='article-content')
                            if nested_content:
                                # æå–åµŒå¥—å†…å®¹çš„å­å…ƒç´ åˆ°çˆ¶çº§
                                nested_children = nested_content.find_all(recursive=False)
                                for child in nested_children:
                                    nested_content.parent.append(child)
                                nested_content.decompose()
                            
                            # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
                            main_content = (content_copy.find('div', class_='original-content') or 
                                          content_copy.find('div', class_='content') or
                                          content_copy)
                            
                            if main_content:
                                inner_html = main_content.decode_contents() if hasattr(main_content, 'decode_contents') else str(main_content)
                            else:
                                inner_html = content_copy.decode_contents() if hasattr(content_copy, 'decode_contents') else str(content_copy)
                            
                            # ä¿®å¤å›¾ç‰‡è·¯å¾„ï¼šä»æ–‡ç« é¡µé¢çš„ç›¸å¯¹è·¯å¾„è°ƒæ•´ä¸ºä¸»é¡µé¢çš„ç›¸å¯¹è·¯å¾„
                            inner_html = self._fix_image_paths_for_index(inner_html)
                            article_content = f"<div class='article-body'>{inner_html}</div>"
                        else:
                            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šå®¹å™¨ï¼Œæå–bodyä¸­çš„ä¸»è¦å†…å®¹
                            body = soup.find('body')
                            if body:
                                # ç§»é™¤å¯¼èˆªã€å¤´éƒ¨ã€è„šéƒ¨ã€è„šæœ¬ç­‰ä¸éœ€è¦çš„å…ƒç´ 
                                for tag in body.find_all(['nav', 'header', 'footer', 'script', 'style', '.navbar', '.back-to-home']):
                                    if tag:
                                        tag.decompose()
                                
                                # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
                                main_content = body.find(['main', 'article', '.content', '.main'])
                                if main_content:
                                    article_content = f"<div class='article-body'>{main_content.decode_contents()}</div>"
                                else:
                                    # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼Œæå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                                    text_content = body.get_text(separator='\n', strip=True)
                                    if len(text_content) > 100:
                                        article_content = f"<div class='article-body'><pre>{text_content[:2000]}...</pre></div>"
                                    else:
                                        article_content = "<div class='article-body'>å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–</div>"
                            else:
                                article_content = "<div class='article-body'>æ— æ³•æ‰¾åˆ°bodyæ ‡ç­¾</div>"
                except Exception as e:
                    article_content = f"<div class='article-body'>å†…å®¹åŠ è½½å‡ºé”™: {e}</div>"
            else:
                article_content = f"<div class='article-body'>æ–‡ç« æ–‡ä»¶æœªæ‰¾åˆ°: {html_file}</div>"
            
            # ç”Ÿæˆæ–‡ç« å†…å®¹HTML
            contents.append(f'''
            <div class="article-content" id="article-{article_id}">
                <div class="article-header">
                    <h1 class="article-title">{article.title}</h1>
                    <div class="article-meta">
                        <span>åˆ†ç±»: {getattr(article, 'category', 'æœªçŸ¥')}</span>
                    </div>
                </div>
                {article_content}
            </div>''')
        
        return ''.join(contents)

    def _fix_article_links(self, articles: List) -> None:
        """ä¿®å¤æ‰€æœ‰æ–‡ç« é—´çš„é“¾æ¥"""
        logger.info("å¼€å§‹ä¿®å¤æ–‡ç« é—´çš„é“¾æ¥...")
        
        # 1. å»ºç«‹æ–‡ç« IDåˆ°æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
        article_map = {}
        for article in articles:
            # æå–æ–‡ç« ID
            import re
            if hasattr(article, 'url') and article.url:
                id_match = re.search(r'/hc/kb/article/(\d+)', article.url)
                if id_match:
                    article_id = id_match.group(1)
                    
                    # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
                    category_parts = getattr(article, 'category', 'å…¶ä»–').split('/')
                    safe_parts = [get_safe_filename(part) for part in category_parts]
                    relative_path = '/'.join(safe_parts)
                    
                    # æ–‡ä»¶åï¼šID_æ ‡é¢˜.html
                    safe_title = get_safe_filename(article.title)
                    filename = f"{article_id}_{safe_title}.html"
                    
                    article_map[article_id] = f"{relative_path}/{filename}"
        
        logger.info(f"å»ºç«‹äº† {len(article_map)} ä¸ªæ–‡ç« çš„è·¯å¾„æ˜ å°„")
        
        # 2. éå†æ‰€æœ‰HTMLæ–‡ä»¶ï¼Œæ›¿æ¢article://é“¾æ¥
        html_files = list(self.html_dir.rglob("*.html"))
        fixed_count = 0
        
        for html_file in html_files:
            if html_file.name == "index.html":
                continue
                
            try:
                with open(html_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                original_content = content
                
                # æ›¿æ¢æ‰€æœ‰article://é“¾æ¥
                import re
                def replace_article_link(match):
                    article_id = match.group(1)
                    if article_id in article_map:
                        # è®¡ç®—ç›¸å¯¹è·¯å¾„
                        current_dir = html_file.parent
                        target_path = self.html_dir / article_map[article_id]
                        
                        # è®¡ç®—ç›¸å¯¹è·¯å¾„
                        try:
                            relative_path = os.path.relpath(target_path, current_dir)
                            relative_path = relative_path.replace('\\', '/')  # Windowsè·¯å¾„è½¬æ¢
                            return f'href="{relative_path}"'
                        except ValueError:
                            # å¦‚æœæ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
                            return f'href="{article_map[article_id]}"'
                    else:
                        # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”æ–‡ç« ï¼Œç”Ÿæˆé¢„æœŸçš„æœ¬åœ°æ–‡ä»¶è·¯å¾„
                        # ä½¿ç”¨ç®€æ´æ ¼å¼ï¼š{article_id}.htmlï¼Œæ”¾åœ¨"å…¶ä»–"åˆ†ç±»ä¸‹
                        expected_path = f"å…¶ä»–/{article_id}.html"
                        try:
                            current_dir = html_file.parent
                            target_path = self.html_dir / expected_path
                            relative_path = os.path.relpath(target_path, current_dir)
                            relative_path = relative_path.replace('\\', '/')  # Windowsè·¯å¾„è½¬æ¢
                            return f'href="{relative_path}"'
                        except ValueError:
                            # å¦‚æœæ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
                            return f'href="{expected_path}"'
                
                # å¤„ç†å„ç§å ä½ç¬¦æ ¼å¼çš„é“¾æ¥ï¼ˆå¦‚æœè¿˜æœ‰çš„è¯ï¼‰
                content = re.sub(r'href="article://(\d+)"', replace_article_link, content)
                content = re.sub(r'href="LOCAL_FILE:(\d+)"', replace_article_link, content)
                content = re.sub(r'href="ARTICLE_ID:(\d+)"', replace_article_link, content)
                
                # å¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼Œä¿å­˜æ–‡ä»¶
                if content != original_content:
                    with open(html_file, 'w', encoding='utf-8') as f:
                        f.write(content)
                    fixed_count += 1
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {html_file} æ—¶å‡ºé”™: {e}")
        
        logger.info(f"ä¿®å¤å®Œæˆï¼Œå…±å¤„ç†äº† {fixed_count} ä¸ªæ–‡ä»¶")

    def _fix_index_html_links(self, index_file: Path, articles: List) -> None:
        """ä¿®å¤index.htmlä¸­çš„æ–‡ç« é“¾æ¥"""
        logger.info("å¼€å§‹ä¿®å¤index.htmlä¸­çš„é“¾æ¥...")
        
        # 1. å»ºç«‹æ–‡ç« IDåˆ°æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
        import re
        article_map = {}
        for article in articles:
            if hasattr(article, 'url') and article.url:
                id_match = re.search(r'/hc/kb/article/(\d+)', article.url)
                if id_match:
                    article_id = id_match.group(1)
                    
                    # ç”Ÿæˆç›¸å¯¹äºindex.htmlçš„æ–‡ä»¶è·¯å¾„
                    category_parts = (getattr(article, 'category', '') or 'å…¶ä»–').split('/')
                    safe_parts = [get_safe_filename(part) for part in category_parts]
                    relative_path = '/'.join(safe_parts)
                    
                    # æ–‡ä»¶åï¼šID_æ ‡é¢˜.html
                    safe_title = get_safe_filename(article.title)
                    filename = f"{article_id}_{safe_title}.html"
                    
                    # index.htmlåœ¨æ ¹ç›®å½•ï¼Œæ‰€ä»¥è·¯å¾„éœ€è¦åŠ ä¸Šhtml/å‰ç¼€
                    article_map[article_id] = f"html/{relative_path}/{filename}"
        
        logger.info(f"å»ºç«‹äº† {len(article_map)} ä¸ªæ–‡ç« çš„è·¯å¾„æ˜ å°„ï¼ˆé’ˆå¯¹index.htmlï¼‰")
        
        # 2. è¯»å–å¹¶ä¿®å¤index.html
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            
            # æ›¿æ¢é“¾æ¥çš„å‡½æ•°
            def replace_link(match):
                article_id = match.group(1)
                if article_id in article_map:
                    return f'href="{article_map[article_id]}"'
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”æ–‡ç« ï¼Œç”Ÿæˆé¢„æœŸçš„æœ¬åœ°æ–‡ä»¶è·¯å¾„
                    # ä½¿ç”¨ç®€æ´æ ¼å¼ï¼šhtml/å…¶ä»–/{article_id}.htmlï¼ˆç›¸å¯¹äºindex.htmlï¼‰
                    expected_path = f"html/å…¶ä»–/{article_id}.html"
                    return f'href="{expected_path}"'
            
            # å¤„ç†å„ç§å ä½ç¬¦æ ¼å¼çš„é“¾æ¥ï¼ˆå¦‚æœè¿˜æœ‰çš„è¯ï¼‰
            content = re.sub(r'href="article://(\d+)"', replace_link, content)
            content = re.sub(r'href="LOCAL_FILE:(\d+)"', replace_link, content)
            content = re.sub(r'href="ARTICLE_ID:(\d+)"', replace_link, content)
            
            # å¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼Œä¿å­˜æ–‡ä»¶
            if content != original_content:
                with open(index_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                logger.info("index.htmlé“¾æ¥ä¿®å¤å®Œæˆ")
            else:
                logger.info("index.htmlæ— éœ€ä¿®å¤")
                
        except Exception as e:
            logger.error(f"ä¿®å¤index.htmlé“¾æ¥æ—¶å‡ºé”™: {e}")

    def _generate_article_list(self, articles: List) -> str:
        """ç”Ÿæˆæ–‡ç« åˆ—è¡¨HTML"""
        items = []
        for article in articles:
            if not hasattr(article, 'title') or not article.title:
                continue

            # ç”Ÿæˆç›¸å¯¹è·¯å¾„ï¼ˆåŒ…å«æ–‡ç« IDå‰ç¼€ï¼‰
            category_parts = getattr(article, 'category', 'å…¶ä»–').split('/')
            relative_path = '/'.join(get_safe_filename(part) for part in category_parts)
            
            # æå–æ–‡ç« ID
            import re
            article_id = ""
            if hasattr(article, 'url') and article.url:
                id_match = re.search(r'/hc/kb/article/(\d+)', article.url)
                if id_match:
                    article_id = id_match.group(1)
            
            safe_title = get_safe_filename(article.title)
            if article_id:
                article_path = f"{relative_path}/{article_id}_{safe_title}.html"
            else:
                article_path = f"{relative_path}/{safe_title}.html"

            items.append(f"""
                <li class="article-item" data-category="{getattr(article, 'category', 'æœªçŸ¥')}">
                    <a href="{article_path}" class="article-title">{article.title}</a>
                </li>
            """)
        return ''.join(items)

        return ''.join(items)
